{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9c06b4f-b1e4-4c59-8c3d-fbf51a652aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "병합 완료! merged_train.csv로 저장됨\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로\n",
    "train_path = r\"C:\\Users\\user\\Downloads\\open (1)\\train.csv\"\n",
    "building_info_path = r\"C:\\Users\\user\\Downloads\\open (1)\\building_info.csv\"\n",
    "\n",
    "# CSV 불러오기\n",
    "train_df = pd.read_csv(train_path)\n",
    "building_info_df = pd.read_csv(building_info_path)\n",
    "\n",
    "# 병합 (건물번호 기준)\n",
    "merged_df = pd.merge(train_df, building_info_df, on='건물번호', how='left')\n",
    "\n",
    "# 결과 저장\n",
    "merged_df.to_csv(r\"C:\\Users\\user\\Downloads\\open (1)\\merged_train.csv\", index=False, encoding = 'cp949')\n",
    "\n",
    "print(\"병합 완료! merged_train.csv로 저장됨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0df8ba4-14a4-41b8-8d2b-61ef686b6a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_smart(path):\n",
    "    import pandas as pd\n",
    "    for enc in ['cp949', 'utf-8-sig', 'utf-8', 'euc-kr']:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    # 최후의 보루: 깨지는 글자는 � 로 대체\n",
    "    return pd.read_csv(path, encoding='utf-8', errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ffaff9-a16e-4dd6-a92e-e47e0ef61da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            일시         날짜  시간\n",
      "0  20240601 00 2024-06-01   0\n",
      "1  20240601 01 2024-06-01   1\n",
      "2  20240601 02 2024-06-01   2\n",
      "3  20240601 03 2024-06-01   3\n",
      "4  20240601 04 2024-06-01   4\n"
     ]
    }
   ],
   "source": [
    "# CSV 불러오기 (예시)\n",
    "df = read_csv_smart(\"C:\\\\Users\\\\user\\\\Downloads\\\\open (1)\\\\merged_train.csv\")\n",
    "\n",
    "\n",
    "# '일시'를 문자열로 변환 후 날짜와 시간 분리\n",
    "df['일시'] = df['일시'].astype(str)\n",
    "\n",
    "# 날짜(YYYYMMDD)와 시간(HH) 분리\n",
    "df['날짜'] = df['일시'].str.slice(0, 8)     # 앞 8자리 → 날짜\n",
    "df['시간'] = df['일시'].str.slice(9, 11)    # 9~10번째 자리 → 시간\n",
    "\n",
    "# 날짜를 datetime 형식으로 변환\n",
    "df['날짜'] = pd.to_datetime(df['날짜'], format='%Y%m%d')\n",
    "df['시간'] = df['시간'].astype(int)\n",
    "\n",
    "# 확인\n",
    "print(df[['일시', '날짜', '시간']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36cc6a8-0547-428e-958d-32a375333aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_date_time</th>\n",
       "      <th>건물번호</th>\n",
       "      <th>일시</th>\n",
       "      <th>기온(°C)</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>풍속(m/s)</th>\n",
       "      <th>습도(%)</th>\n",
       "      <th>일조(hr)</th>\n",
       "      <th>일사(MJ/m2)</th>\n",
       "      <th>전력소비량(kWh)</th>\n",
       "      <th>건물유형</th>\n",
       "      <th>연면적(m2)</th>\n",
       "      <th>냉방면적(m2)</th>\n",
       "      <th>태양광용량(kW)</th>\n",
       "      <th>ESS저장용량(kWh)</th>\n",
       "      <th>PCS용량(kW)</th>\n",
       "      <th>날짜</th>\n",
       "      <th>시간</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_20240601 00</td>\n",
       "      <td>1</td>\n",
       "      <td>20240601 00</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5794.80</td>\n",
       "      <td>호텔</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_20240601 01</td>\n",
       "      <td>1</td>\n",
       "      <td>20240601 01</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5591.85</td>\n",
       "      <td>호텔</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_20240601 02</td>\n",
       "      <td>1</td>\n",
       "      <td>20240601 02</td>\n",
       "      <td>18.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5338.17</td>\n",
       "      <td>호텔</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_20240601 03</td>\n",
       "      <td>1</td>\n",
       "      <td>20240601 03</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4554.42</td>\n",
       "      <td>호텔</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_20240601 04</td>\n",
       "      <td>1</td>\n",
       "      <td>20240601 04</td>\n",
       "      <td>17.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3602.25</td>\n",
       "      <td>호텔</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203995</th>\n",
       "      <td>100_20240824 19</td>\n",
       "      <td>100</td>\n",
       "      <td>20240824 19</td>\n",
       "      <td>29.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.18</td>\n",
       "      <td>3276.00</td>\n",
       "      <td>호텔</td>\n",
       "      <td>162070.24</td>\n",
       "      <td>152943.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2024-08-24</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203996</th>\n",
       "      <td>100_20240824 20</td>\n",
       "      <td>100</td>\n",
       "      <td>20240824 20</td>\n",
       "      <td>28.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3197.52</td>\n",
       "      <td>호텔</td>\n",
       "      <td>162070.24</td>\n",
       "      <td>152943.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2024-08-24</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203997</th>\n",
       "      <td>100_20240824 21</td>\n",
       "      <td>100</td>\n",
       "      <td>20240824 21</td>\n",
       "      <td>28.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3006.60</td>\n",
       "      <td>호텔</td>\n",
       "      <td>162070.24</td>\n",
       "      <td>152943.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2024-08-24</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203998</th>\n",
       "      <td>100_20240824 22</td>\n",
       "      <td>100</td>\n",
       "      <td>20240824 22</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2649.72</td>\n",
       "      <td>호텔</td>\n",
       "      <td>162070.24</td>\n",
       "      <td>152943.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2024-08-24</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203999</th>\n",
       "      <td>100_20240824 23</td>\n",
       "      <td>100</td>\n",
       "      <td>20240824 23</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2929.32</td>\n",
       "      <td>호텔</td>\n",
       "      <td>162070.24</td>\n",
       "      <td>152943.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2024-08-24</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204000 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          num_date_time  건물번호           일시  기온(°C)  강수량(mm)  풍속(m/s)  습도(%)  \\\n",
       "0         1_20240601 00     1  20240601 00    18.3      0.0      2.6   82.0   \n",
       "1         1_20240601 01     1  20240601 01    18.3      0.0      2.7   82.0   \n",
       "2         1_20240601 02     1  20240601 02    18.1      0.0      2.6   80.0   \n",
       "3         1_20240601 03     1  20240601 03    18.0      0.0      2.6   81.0   \n",
       "4         1_20240601 04     1  20240601 04    17.8      0.0      1.3   81.0   \n",
       "...                 ...   ...          ...     ...      ...      ...    ...   \n",
       "203995  100_20240824 19   100  20240824 19    29.1      0.0      4.4   76.0   \n",
       "203996  100_20240824 20   100  20240824 20    28.6      0.0      3.7   74.0   \n",
       "203997  100_20240824 21   100  20240824 21    28.3      0.0      2.9   74.0   \n",
       "203998  100_20240824 22   100  20240824 22    28.0      0.0      1.7   76.0   \n",
       "203999  100_20240824 23   100  20240824 23    28.0      0.0      2.1   75.0   \n",
       "\n",
       "        일조(hr)  일사(MJ/m2)  전력소비량(kWh) 건물유형    연면적(m2)  냉방면적(m2) 태양광용량(kW)  \\\n",
       "0          0.0       0.00     5794.80   호텔   82912.71   77586.0         -   \n",
       "1          0.0       0.00     5591.85   호텔   82912.71   77586.0         -   \n",
       "2          0.0       0.00     5338.17   호텔   82912.71   77586.0         -   \n",
       "3          0.0       0.00     4554.42   호텔   82912.71   77586.0         -   \n",
       "4          0.0       0.00     3602.25   호텔   82912.71   77586.0         -   \n",
       "...        ...        ...         ...  ...        ...       ...       ...   \n",
       "203995     0.4       0.18     3276.00   호텔  162070.24  152943.0         -   \n",
       "203996     0.0       0.00     3197.52   호텔  162070.24  152943.0         -   \n",
       "203997     0.0       0.00     3006.60   호텔  162070.24  152943.0         -   \n",
       "203998     0.0       0.00     2649.72   호텔  162070.24  152943.0         -   \n",
       "203999     0.0       0.00     2929.32   호텔  162070.24  152943.0         -   \n",
       "\n",
       "       ESS저장용량(kWh) PCS용량(kW)         날짜  시간  \n",
       "0                 -         - 2024-06-01   0  \n",
       "1                 -         - 2024-06-01   1  \n",
       "2                 -         - 2024-06-01   2  \n",
       "3                 -         - 2024-06-01   3  \n",
       "4                 -         - 2024-06-01   4  \n",
       "...             ...       ...        ...  ..  \n",
       "203995            -         - 2024-08-24  19  \n",
       "203996            -         - 2024-08-24  20  \n",
       "203997            -         - 2024-08-24  21  \n",
       "203998            -         - 2024-08-24  22  \n",
       "203999            -         - 2024-08-24  23  \n",
       "\n",
       "[204000 rows x 18 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bae45f3-a91e-4565-875e-fd288783f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 대체할 컬럼 목록\n",
    "cols = ['태양광용량(kW)', 'ESS저장용량(kWh)', 'PCS용량(kW)']\n",
    "\n",
    "# '-'를 0으로 바꾸고 숫자형으로 변환\n",
    "for col in cols:\n",
    "    df[col] = df[col].replace('-', 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "217e50e1-22ea-4604-b35e-1d81d9159c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['num_date_time', '일시'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a20ba33f-8d72-4347-9553-0f986dffdae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['건물유형'] = le.fit_transform(df['건물유형'])\n",
    "df['날짜'] = pd.to_datetime(df['날짜'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e97aec1-f316-432b-8324-8ff9ae39c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 0) 기본 정렬 & datetime 만들기\n",
    "    #    (이미 df['날짜']와 df['시간']이 있다면 그대로 쓰되, 한 줄짜리 datetime을 만들어두면 편함)\n",
    "    df = df.copy()\n",
    "    df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "    df['dt'] = df['날짜'] + pd.to_timedelta(df['시간'], unit='h')\n",
    "    df = df.sort_values(['건물번호', 'dt']).reset_index(drop=True)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 1) 최근 24시간 평균, 최근 7일(같은 시각) 평균  → 전부 \"과거만\" 보도록 shift 사용\n",
    "    # -------------------------------------------------------\n",
    "    grp = df.groupby('건물번호', group_keys=False)\n",
    "\n",
    "    # (a) 최근 24시간 평균 (전력소비량 기준)\n",
    "    #  - window=24, past-only를 위해 shift(1) 후 rolling\n",
    "    df['cons_lag1'] = grp['전력소비량(kWh)'].shift(1)\n",
    "    df['cons_mean_24h'] = grp['cons_lag1'].rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "    # (b) 최근 7일 같은 시각 평균 (24시간 간격으로 7개)\n",
    "    #  - 1일 전 같은 시각부터 7일 전 같은 시각까지 평균\n",
    "    same_hour_lag = grp['전력소비량(kWh)'].shift(24)\n",
    "    df['cons_samehour_mean_7d'] = same_hour_lag.rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "    # 참고로 모델에 바로 쓰진 않아도 되는 추가 라그들(원하면 활성화)\n",
    "    df['cons_lag_24h'] = grp['전력소비량(kWh)'].shift(24)\n",
    "    df['cons_lag_48h'] = grp['전력소비량(kWh)'].shift(48)\n",
    "    df['cons_lag_72h'] = grp['전력소비량(kWh)'].shift(72)\n",
    "    df['cons_lag_168h'] = grp['전력소비량(kWh)'].shift(168)  # 7일 전 같은 시각\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2) 기온·일사 기반 냉방 수요 지표 (CDD류)\n",
    "    # -------------------------------------------------------\n",
    "    # 한국 여름 기준 base temp 24°C 가합리(필요시 23~26으로 튜닝)\n",
    "    base_temp = 24.0\n",
    "    # ‘냉방도수’(Cooling Degree) 시간 단위\n",
    "    df['CDD'] = (df['기온(°C)'] - base_temp).clip(lower=0)\n",
    "\n",
    "    # 일사량(MJ/m2)과의 상호작용: 햇볕이 강할수록 체감 부하↑\n",
    "    # 일사량이 0~상위 99퍼센타일 사이로 정규화(robust)\n",
    "    q99 = df['일사(MJ/m2)'].quantile(0.99)\n",
    "    rad_norm = (df['일사(MJ/m2)'].clip(upper=q99) / (q99 + 1e-6))\n",
    "    df['CDD_x_rad'] = df['CDD'] * rad_norm\n",
    "\n",
    "    # 습도(%)와의 상호작용: 습도가 높으면 동일 온도에서도 냉방 부하↑\n",
    "    # 간단히 (1 + 습도/100*알파) 가중. 알파=0.3 정도로 시작(튜닝 가능)\n",
    "    alpha = 0.3\n",
    "    df['CDD_humid_adj'] = df['CDD'] * (1 + alpha * (df['습도(%)'] / 100.0))\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3) 주말/평일, 공휴일\n",
    "    # -------------------------------------------------------\n",
    "    df['weekday'] = df['dt'].dt.weekday  # 월=0 ... 일=6\n",
    "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
    "\n",
    "    # 2024-06~08 사이 한국 공휴일: 현충일(6/6), 광복절(8/15)\n",
    "    kr_holidays = {\n",
    "        pd.Timestamp(2024, 6, 6),  # 현충일\n",
    "        pd.Timestamp(2024, 8, 15), # 광복절\n",
    "    }\n",
    "    df['is_holiday'] = df['날짜'].isin(kr_holidays).astype(int)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 4) 태양광·ESS·PCS 용량 대비 “동작 가능성” 지표\n",
    "    #    (실제 제어 로그가 없으니 ‘가능성/잠재력’을 피처로 넣는다)\n",
    "    # -------------------------------------------------------\n",
    "    # 설비 유무 이진\n",
    "    df['has_pv'] = (df['태양광용량(kW)'] > 0).astype(int)\n",
    "    df['has_ess'] = (df['ESS저장용량(kWh)'] > 0).astype(int)\n",
    "    df['has_pcs'] = (df['PCS용량(kW)'] > 0).astype(int)\n",
    "\n",
    "    # 낮/밤 플래그 (대략 일사량>0이면 주간으로 간주)\n",
    "    df['is_daylight'] = (df['일사(MJ/m2)'] > 0).astype(int)\n",
    "\n",
    "    # 태양광 ‘동작 가능성’ (설비 있고 + 주간/일사>0)\n",
    "    df['pv_active_potential'] = ((df['has_pv'] == 1) & (df['is_daylight'] == 1)).astype(int)\n",
    "\n",
    "    # 피크/오프피크 (현실 요금제와 다를 수 있지만 합리적 초기값)\n",
    "    df['is_offpeak'] = df['시간'].isin([0,1,2,3,4,5,6,23]).astype(int)\n",
    "    df['is_peak']    = df['시간'].isin([13,14,15,16,17]).astype(int)\n",
    "\n",
    "    # ESS 충방전 ‘가능성’ 피처\n",
    "    df['ess_charge_potential']   = ((df['has_ess']==1) & (df['is_offpeak']==1)).astype(int)\n",
    "    df['ess_discharge_potential'] = ((df['has_ess']==1) & (df['is_peak']==1)).astype(int)\n",
    "\n",
    "    # 용량 스케일 자체도 피처로 사용(로그 스케일로 완만화; 0은 0으로 유지)\n",
    "    for c in ['태양광용량(kW)', 'ESS저장용량(kWh)', 'PCS용량(kW)']:\n",
    "        df[f'log1p_{c}'] = np.log1p(df[c])\n",
    "\n",
    "    # 누설 방지: 타깃 기반 비율은 과거 라그로만 계산\n",
    "    # ESS 대비 부하 비율(전일 같은 시각 소비량 사용)\n",
    "    df['ess_to_load_lag_ratio'] = np.where(\n",
    "        df['cons_lag_24h'].notna(),\n",
    "        df['ESS저장용량(kWh)'] / (df['cons_lag_24h'] + 1e-6),\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 5) 기타 유틸리티 파생\n",
    "    # -------------------------------------------------------\n",
    "    df['month'] = df['dt'].dt.month\n",
    "    df['hour']  = df['시간']  # 가독성용 복사\n",
    "    df['dayofyear'] = df['dt'].dt.dayofyear\n",
    "\n",
    "    # 모델 입력 전에 의미 없는 원본(또는 중복) 컬럼 정리 원하면 아래 사용\n",
    "    # drop_cols = ['dt']  # 학습 시 굳이 안 써도 되면 제거\n",
    "    # df = df.drop(columns=drop_cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 사용 예시:\n",
    "# df_feat = make_features(df)\n",
    "# df_feat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74e42062-9149-4821-a4de-71cf27598b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0. 라이브러리 & 경로 설정\n",
    "# =========================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pip install lightgbm 먼저 (처음 1번만)\n",
    "# pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "DATA_DIR = r\"C:\\Users\\user\\Downloads\\open (1)\"\n",
    "TRAIN_MERGED_PATH = os.path.join(DATA_DIR, \"merged_train.csv\")\n",
    "TEST_PATH  = os.path.join(DATA_DIR, \"test.csv\")\n",
    "BUILD_PATH = os.path.join(DATA_DIR, \"building_info.csv\")\n",
    "SAMPLE_SUB = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "OUT_SUB    = os.path.join(DATA_DIR, \"baseline_lgbm_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdfde07e-bd40-4c7b-87a9-8aabf725a997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17276\\3382916.py:10: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cols = [c for c in cols if is_numeric_dtype(df[c]) or is_categorical_dtype(df[c])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttrain's rmse: 173.366\tvalid's rmse: 230.791\n",
      "[400]\ttrain's rmse: 154.914\tvalid's rmse: 221.188\n",
      "[600]\ttrain's rmse: 143.383\tvalid's rmse: 216.656\n",
      "[800]\ttrain's rmse: 134.649\tvalid's rmse: 214.773\n",
      "[1000]\ttrain's rmse: 127.614\tvalid's rmse: 213.857\n",
      "[1200]\ttrain's rmse: 121.745\tvalid's rmse: 213.28\n",
      "[1400]\ttrain's rmse: 116.665\tvalid's rmse: 213.123\n",
      "[1600]\ttrain's rmse: 112.139\tvalid's rmse: 212.795\n",
      "[1800]\ttrain's rmse: 108.081\tvalid's rmse: 212.451\n",
      "Early stopping, best iteration is:\n",
      "[1739]\ttrain's rmse: 109.199\tvalid's rmse: 212.38\n",
      "VALID RMSE: 212.38019335781\n",
      "VALID MAE : 101.67560328341894\n"
     ]
    }
   ],
   "source": [
    "# ===== IMPORTS (필요시 중복 있어도 무방) =====\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
    "\n",
    "# ===== 피처 선택 헬퍼: 숫자/카테고리만 남기기 + 불필요 컬럼 드롭 =====\n",
    "def get_feature_cols(df: pd.DataFrame) -> list:\n",
    "    base_drop = ['전력소비량(kWh)', 'dt', '날짜', '시간', '일시', 'num_date_time']\n",
    "    cols = [c for c in df.columns if c not in base_drop]\n",
    "    cols = [c for c in cols if is_numeric_dtype(df[c]) or is_categorical_dtype(df[c])]\n",
    "    return cols\n",
    "\n",
    "# =========================\n",
    "# 1. 유틸 함수들\n",
    "# =========================\n",
    "def ensure_datetime_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"일시 → 날짜/시간 분리(또는 이미 분리돼 있으면 그대로) + dt 생성\"\"\"\n",
    "    df = df.copy()\n",
    "    if '날짜' not in df.columns or '시간' not in df.columns:\n",
    "        if '일시' in df.columns:\n",
    "            s = df['일시'].astype(str)\n",
    "            df['날짜'] = pd.to_datetime(s.str.slice(0, 8), format='%Y%m%d')\n",
    "            df['시간'] = s.str.slice(9, 11).astype(int)\n",
    "        else:\n",
    "            raise ValueError(\"날짜/시간 정보가 없습니다. ('일시' 또는 '날짜','시간' 필요)\")\n",
    "    else:\n",
    "        df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "        df['시간'] = df['시간'].astype(int)\n",
    "    df['dt'] = df['날짜'] + pd.to_timedelta(df['시간'], unit='h')\n",
    "    return df\n",
    "\n",
    "def clean_capacity_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"설비 용량에 '-'가 있으면 0으로 치환 후 float 변환\"\"\"\n",
    "    df = df.copy()\n",
    "    cols = ['태양광용량(kW)', 'ESS저장용량(kWh)', 'PCS용량(kW)']\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].replace('-', 0).astype(float)\n",
    "    return df\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# =========================\n",
    "# 2. 특징 엔지니어링(수정 버전)\n",
    "# =========================\n",
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - 미래정보 누설 방지: 전부 shift/rolling(=transform)로 과거만 사용\n",
    "    - groupby().rolling() 대신 groupby().transform(...) 사용\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 일사/일조 안전 처리(테스트에 없을 수 있음)\n",
    "    if '일사(MJ/m2)' not in df.columns:\n",
    "        df['일사(MJ/m2)'] = 0.0\n",
    "    if '일조(hr)' not in df.columns:\n",
    "        df['일조(hr)'] = 0.0\n",
    "\n",
    "    # 정렬\n",
    "    df = df.sort_values(['건물번호', 'dt']).reset_index(drop=True)\n",
    "\n",
    "    # 그룹 객체\n",
    "    grp = df.groupby('건물번호', sort=False)\n",
    "\n",
    "    # ---------- 타깃 라그 ----------\n",
    "    if '전력소비량(kWh)' in df.columns:\n",
    "        df['cons_lag1']     = grp['전력소비량(kWh)'].shift(1)\n",
    "        df['cons_lag_24h']  = grp['전력소비량(kWh)'].shift(24)\n",
    "        df['cons_lag_48h']  = grp['전력소비량(kWh)'].shift(48)\n",
    "        df['cons_lag_72h']  = grp['전력소비량(kWh)'].shift(72)\n",
    "        df['cons_lag_168h'] = grp['전력소비량(kWh)'].shift(168)  # 7일 전\n",
    "        # 최근 24시간 평균 (과거만 참고)\n",
    "        df['cons_mean_24h'] = grp['전력소비량(kWh)'] \\\n",
    "            .transform(lambda s: s.shift(1).rolling(window=24, min_periods=1).mean())\n",
    "        # 최근 7일 같은 시각 평균\n",
    "        df['cons_samehour_mean_7d'] = grp['전력소비량(kWh)'] \\\n",
    "            .transform(lambda s: s.shift(24).rolling(window=7, min_periods=1).mean())\n",
    "    else:\n",
    "        for c in ['cons_lag1','cons_lag_24h','cons_lag_48h','cons_lag_72h','cons_lag_168h',\n",
    "                  'cons_mean_24h','cons_samehour_mean_7d']:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    # ---------- 냉방 수요 지표 ----------\n",
    "    base_temp = 24.0\n",
    "    df['CDD'] = (df['기온(°C)'] - base_temp).clip(lower=0)\n",
    "\n",
    "    q99 = df['일사(MJ/m2)'].quantile(0.99) + 1e-6\n",
    "    rad_norm = (df['일사(MJ/m2)'].clip(upper=q99) / q99)\n",
    "    df['CDD_x_rad'] = df['CDD'] * rad_norm\n",
    "\n",
    "    alpha = 0.3\n",
    "    df['CDD_humid_adj'] = df['CDD'] * (1 + alpha * (df['습도(%)'] / 100.0))\n",
    "\n",
    "    # ---------- 달력 피처 ----------\n",
    "    df['weekday']    = df['dt'].dt.weekday\n",
    "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
    "    kr_holidays = {pd.Timestamp(2024,6,6), pd.Timestamp(2024,8,15)}\n",
    "    df['is_holiday'] = df['날짜'].isin(kr_holidays).astype(int)\n",
    "\n",
    "    # ---------- 설비 가능성 피처 ----------\n",
    "    df['has_pv']  = (df['태양광용량(kW)']  > 0).astype(int)\n",
    "    df['has_ess'] = (df['ESS저장용량(kWh)'] > 0).astype(int)\n",
    "    df['has_pcs'] = (df['PCS용량(kW)']    > 0).astype(int)\n",
    "    df['is_daylight'] = (df['일사(MJ/m2)'] > 0).astype(int)\n",
    "\n",
    "    df['is_offpeak'] = df['시간'].isin([0,1,2,3,4,5,6,23]).astype(int)\n",
    "    df['is_peak']    = df['시간'].isin([13,14,15,16,17]).astype(int)\n",
    "\n",
    "    df['ess_charge_potential']    = ((df['has_ess']==1) & (df['is_offpeak']==1)).astype(int)\n",
    "    df['ess_discharge_potential'] = ((df['has_ess']==1) & (df['is_peak']==1)).astype(int)\n",
    "\n",
    "    for c in ['태양광용량(kW)', 'ESS저장용량(kWh)', 'PCS용량(kW)']:\n",
    "        df[f'log1p_{c}'] = np.log1p(df[c])\n",
    "\n",
    "    df['ess_to_load_lag_ratio'] = np.where(\n",
    "        df.get('cons_lag_24h', pd.Series(index=df.index)).notna(),\n",
    "        df['ESS저장용량(kWh)'] / (df['cons_lag_24h'] + 1e-6),\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # ---------- 기타 ----------\n",
    "    df['month'] = df['dt'].dt.month\n",
    "    df['hour']  = df['시간']\n",
    "    df['dayofyear'] = df['dt'].dt.dayofyear\n",
    "\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# 3. 학습 데이터 로드 & 피처 생성\n",
    "# =========================\n",
    "train_df = read_csv_smart(TRAIN_MERGED_PATH)\n",
    "train_df = clean_capacity_fields(train_df)\n",
    "train_df = ensure_datetime_cols(train_df)\n",
    "train_feat = make_features(train_df)\n",
    "\n",
    "# =========================\n",
    "# 4. 시계열 검증 분할 (2024-08-17 ~ 2024-08-24)\n",
    "# =========================\n",
    "VAL_START = pd.Timestamp(2024,8,17,0)\n",
    "VAL_END   = pd.Timestamp(2024,8,24,23)\n",
    "\n",
    "is_val = (train_feat['dt'] >= VAL_START) & (train_feat['dt'] <= VAL_END)\n",
    "train_part = train_feat[~is_val].copy()\n",
    "valid_part = train_feat[ is_val].copy()\n",
    "\n",
    "# === 피처 선택 (object/문자열 배제) ===\n",
    "features = get_feature_cols(train_feat)\n",
    "target_col = '전력소비량(kWh)'\n",
    "\n",
    "# 카테고리 지정(피처에 포함된 컬럼만)\n",
    "cat_cols = [c for c in ['건물번호','건물유형'] if c in features]\n",
    "for c in cat_cols:\n",
    "    train_part[c] = train_part[c].astype('category')\n",
    "    valid_part[c] = valid_part[c].astype('category')\n",
    "\n",
    "X_tr, y_tr = train_part[features], train_part[target_col]\n",
    "X_va, y_va = valid_part[features], valid_part[target_col]\n",
    "\n",
    "# =========================\n",
    "# 5. LightGBM 학습 & 검증 (콜백 방식 조기종료)\n",
    "# =========================\n",
    "lgb_train = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_cols or None)\n",
    "lgb_valid = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_cols or None)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'feature_fraction': 0.85,\n",
    "    'bagging_fraction': 0.85,\n",
    "    'bagging_freq': 1,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'lambda_l2': 1.0,\n",
    "    'seed': 42,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=200),\n",
    "    lgb.log_evaluation(period=200)\n",
    "]\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=5000,\n",
    "    valid_sets=[lgb_train, lgb_valid],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "pred_va = model.predict(X_va, num_iteration=model.best_iteration)\n",
    "print(\"VALID RMSE:\", rmse(y_va, pred_va))\n",
    "print(\"VALID MAE :\", mean_absolute_error(y_va, pred_va))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "060ab07e-197a-4cc9-8ad4-973bcffd8303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backfill_solar_by_time(all_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # dt/시간 보장\n",
    "    if 'dt' not in all_df.columns:\n",
    "        raise ValueError(\"dt 없으면 ensure_datetime_cols 먼저 호출\")\n",
    "    all_df = all_df.copy()\n",
    "    all_df['month'] = all_df['dt'].dt.month\n",
    "    all_df['hour']  = all_df['시간']\n",
    "\n",
    "    # train/test 구분: 타깃 존재 여부로 판별\n",
    "    is_train = all_df['전력소비량(kWh)'].notna() if '전력소비량(kWh)' in all_df.columns else pd.Series(False, index=all_df.index)\n",
    "\n",
    "    # 기준 통계 (train에서만)\n",
    "    rad_ref = all_df.loc[is_train].groupby(['month','hour'])['일사(MJ/m2)'].median()\n",
    "    sun_ref = all_df.loc[is_train].groupby(['month','hour'])['일조(hr)'].median()\n",
    "\n",
    "    # index 매칭해서 test 행만 채움\n",
    "    idx = pd.MultiIndex.from_frame(all_df[['month','hour']])\n",
    "    fill_rad = rad_ref.reindex(idx).values\n",
    "    fill_sun = sun_ref.reindex(idx).values\n",
    "\n",
    "    need_fill_rad = (~is_train) & (all_df['일사(MJ/m2)'].isna() | (all_df['일사(MJ/m2)'] == 0))\n",
    "    need_fill_sun = (~is_train) & (all_df['일조(hr)'].isna()     | (all_df['일조(hr)'] == 0))\n",
    "\n",
    "    all_df.loc[need_fill_rad, '일사(MJ/m2)'] = fill_rad[need_fill_rad.values]\n",
    "    all_df.loc[need_fill_sun, '일조(hr)']    = fill_sun[need_fill_sun.values]\n",
    "\n",
    "    # 혹시라도 남은 결측은 0으로\n",
    "    all_df['일사(MJ/m2)'] = all_df['일사(MJ/m2)'].fillna(0)\n",
    "    all_df['일조(hr)']    = all_df['일조(hr)'].fillna(0)\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bdf3f-45d2-41a1-8381-5cdc397ec9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9cfa6fe-1aa4-496b-9b7e-1dbbd032e74d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# train + test concat\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m all_df = pd.concat([train_df, \u001b[43mtest_df\u001b[49m], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m all_df = clean_capacity_fields(all_df)\n\u001b[32m      4\u001b[39m all_df = ensure_datetime_cols(all_df)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "# train + test concat\n",
    "all_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "all_df = clean_capacity_fields(all_df)\n",
    "all_df = ensure_datetime_cols(all_df)\n",
    "\n",
    "# ★ 추가\n",
    "all_df = backfill_solar_by_time(all_df)\n",
    "\n",
    "# 그 다음 피처 생성\n",
    "all_feat = make_features(all_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301bc87-5bc8-4aaf-805c-7ed92579fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_features() 끝부분에 추가\n",
    "df['hour_sin'] = np.sin(2*np.pi*df['시간']/24)\n",
    "df['hour_cos'] = np.cos(2*np.pi*df['시간']/24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89501477-9c9f-4706-bc84-ebd15a9e5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - 미래정보 누설 방지: shift/rolling 모두 과거만 사용\n",
    "    - groupby().transform(...) 으로 인덱스 정렬 유지\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 일사/일조 안전 처리(테스트에 없을 수 있음)\n",
    "    if '일사(MJ/m2)' not in df.columns:\n",
    "        df['일사(MJ/m2)'] = 0.0\n",
    "    if '일조(hr)' not in df.columns:\n",
    "        df['일조(hr)'] = 0.0\n",
    "\n",
    "    # 정렬 후 그룹 객체 생성\n",
    "    df = df.sort_values(['건물번호', 'dt']).reset_index(drop=True)\n",
    "    grp = df.groupby('건물번호', sort=False)  # ← 여기서 grp 정의\n",
    "\n",
    "    # ---------- 타깃 라그 & 롤링 ----------\n",
    "    if '전력소비량(kWh)' in df.columns:\n",
    "        df['cons_lag1']     = grp['전력소비량(kWh)'].shift(1)\n",
    "        df['cons_lag_24h']  = grp['전력소비량(kWh)'].shift(24)\n",
    "        df['cons_lag_48h']  = grp['전력소비량(kWh)'].shift(48)\n",
    "        df['cons_lag_72h']  = grp['전력소비량(kWh)'].shift(72)\n",
    "        df['cons_lag_168h'] = grp['전력소비량(kWh)'].shift(168)  # 7일 전\n",
    "\n",
    "        # 최근 24시간 평균(과거만; shift(1) 후 rolling)\n",
    "        df['cons_mean_24h'] = grp['전력소비량(kWh)'] \\\n",
    "            .transform(lambda s: s.shift(1).rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        # 최근 7일 같은 시각 평균(24시간 간격 7개)\n",
    "        df['cons_samehour_mean_7d'] = grp['전력소비량(kWh)'] \\\n",
    "            .transform(lambda s: s.shift(24).rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        # ✅ 최근 24시간 표준편차(변동성) — cons_lag1(과거값) 기반\n",
    "        df['cons_std_24h'] = grp['cons_lag1'] \\\n",
    "            .transform(lambda s: s.rolling(window=24, min_periods=6).std())\n",
    "    else:\n",
    "        for c in ['cons_lag1','cons_lag_24h','cons_lag_48h','cons_lag_72h','cons_lag_168h',\n",
    "                  'cons_mean_24h','cons_samehour_mean_7d','cons_std_24h']:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    # ---------- 냉방 수요 지표 ----------\n",
    "    base_temp = 24.0\n",
    "    df['CDD'] = (df['기온(°C)'] - base_temp).clip(lower=0)\n",
    "\n",
    "    q99 = df['일사(MJ/m2)'].quantile(0.99) + 1e-6\n",
    "    rad_norm = (df['일사(MJ/m2)'].clip(upper=q99) / q99)\n",
    "    df['CDD_x_rad'] = df['CDD'] * rad_norm\n",
    "\n",
    "    alpha = 0.3\n",
    "    df['CDD_humid_adj'] = df['CDD'] * (1 + alpha * (df['습도(%)'] / 100.0))\n",
    "\n",
    "    # ---------- 달력 피처 ----------\n",
    "    df['weekday']    = df['dt'].dt.weekday\n",
    "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
    "    kr_holidays = {pd.Timestamp(2024,6,6), pd.Timestamp(2024,8,15)}\n",
    "    df['is_holiday'] = df['날짜'].isin(kr_holidays).astype(int)\n",
    "\n",
    "    # ---------- 설비 가능성 피처 ----------\n",
    "    df['has_pv']  = (df['태양광용량(kW)']  > 0).astype(int)\n",
    "    df['has_ess'] = (df['ESS저장용량(kWh)'] > 0).astype(int)\n",
    "    df['has_pcs'] = (df['PCS용량(kW)']    > 0).astype(int)\n",
    "    df['is_daylight'] = (df['일사(MJ/m2)'] > 0).astype(int)\n",
    "    df['is_offpeak'] = df['시간'].isin([0,1,2,3,4,5,6,23]).astype(int)\n",
    "    df['is_peak']    = df['시간'].isin([13,14,15,16,17]).astype(int)\n",
    "    df['ess_charge_potential']    = ((df['has_ess']==1) & (df['is_offpeak']==1)).astype(int)\n",
    "    df['ess_discharge_potential'] = ((df['has_ess']==1) & (df['is_peak']==1)).astype(int)\n",
    "\n",
    "    for c in ['태양광용량(kW)', 'ESS저장용량(kWh)', 'PCS용량(kW)']:\n",
    "        df[f'log1p_{c}'] = np.log1p(df[c])\n",
    "\n",
    "    df['ess_to_load_lag_ratio'] = np.where(\n",
    "        df.get('cons_lag_24h', pd.Series(index=df.index)).notna(),\n",
    "        df['ESS저장용량(kWh)'] / (df['cons_lag_24h'] + 1e-6),\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # ---------- 기타 ----------\n",
    "    df['month'] = df['dt'].dt.month\n",
    "    df['hour']  = df['시간']\n",
    "    df['dayofyear'] = df['dt'].dt.dayofyear\n",
    "\n",
    "    # (옵션) 시간 사이클릭 인코딩 원하면 활성화\n",
    "    # df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "    # df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65439c9-850b-44c1-a451-17dd807f64d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'tweedie',\n",
    "    'tweedie_variance_power': 1.4,  # 1.2~1.6 사이 튜닝\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'feature_fraction': 0.85,\n",
    "    'bagging_fraction': 0.85,\n",
    "    'bagging_freq': 1,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'lambda_l2': 1.0,\n",
    "    'seed': 42,\n",
    "    'verbosity': -1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f2280-b543-4aae-b746-c922f6dacc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ABS = 1e12  # 과도한 값 하드 클립 임계\n",
    "\n",
    "def sanitize_matrix(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Inf 제거 + 과대값 클립 (LightGBM은 NaN은 허용, Inf는 불가)\"\"\"\n",
    "    X = X.copy()\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    X[num_cols] = X[num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    X[num_cols] = X[num_cols].clip(lower=-MAX_ABS, upper=MAX_ABS)\n",
    "    return X\n",
    "\n",
    "def safe_log1p_vec(a):\n",
    "    \"\"\"음수/비정상값 방지 후 log1p\"\"\"\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    # 비정상(y에 NaN/inf) → 0으로 대체 (혹은 np.nan 유지하고 마스킹하려면 전략 바꿔도 됨)\n",
    "    a = np.where(np.isfinite(a), a, 0.0)\n",
    "    a = np.clip(a, 0, None)  # -0 방지\n",
    "    return np.log1p(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce77c13-0511-4daf-91f3-d3fbed87993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- a, b 동시 추정(최소제곱) -----\n",
    "pred_va_resid = model.predict(X_va, num_iteration=model.best_iteration)\n",
    "true_resid_va = (y_va - baseline_va).values\n",
    "\n",
    "A = np.column_stack([pred_va_resid, np.ones_like(pred_va_resid)])\n",
    "a, b = np.linalg.lstsq(A, true_resid_va, rcond=None)[0]\n",
    "a = float(np.clip(a, 0.0, 1.5))  # 안정화\n",
    "b = float(b)\n",
    "\n",
    "pred_va = baseline_va.values + a * pred_va_resid + b\n",
    "\n",
    "print(f\"[Blending] a={a:.3f}, b={b:.3f}\")\n",
    "print(\"Baseline-only  RMSE:\", rmse(y_va, baseline_va), \"MAE:\", mean_absolute_error(y_va, baseline_va))\n",
    "print(\"BLENDED       RMSE:\", rmse(y_va, pred_va), \"MAE:\", mean_absolute_error(y_va, pred_va))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f6f66-92fe-4814-a884-d4dc27d9979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잔차 예측\n",
    "test_pred_resid = final_model.predict(X_te, num_iteration=final_model.best_iteration)\n",
    "\n",
    "# 검증에서 구한 a,b로 복원\n",
    "a_use = a if 'a' in globals() else 1.0\n",
    "b_use = b if 'b' in globals() else 0.0\n",
    "test_pred = baseline_te.values + a_use * test_pred_resid + b_use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a9b591-0ea2-466a-9133-70f6fd47967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HARD_BLD = {10,79,3,45,23,1,12,69,64,34}\n",
    "w_tr = np.ones(len(train_part), dtype=float)\n",
    "w_tr[train_part['건물번호'].isin(HARD_BLD).values] = 1.8  # 1.5~2.0 사이 시도\n",
    "\n",
    "lgb_train = lgb.Dataset(X_tr, label=y_tr_resid, weight=w_tr, categorical_feature=cat_cols or None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ac206a-8e4c-40bd-8369-83c3eec78f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) make_features() 셀(패치 포함) 실행\n",
    "\n",
    "# 2) 피처 다시 생성\n",
    "train_feat = make_features(train_df)\n",
    "\n",
    "# 3) 확인\n",
    "print({\n",
    "  'cons_lag1': 'cons_lag1' in train_feat.columns,\n",
    "  'cons_lag_24h': 'cons_lag_24h' in train_feat.columns,\n",
    "  'cons_lag_168h': 'cons_lag_168h' in train_feat.columns,\n",
    "  'delta_1h': 'delta_1h' in train_feat.columns,\n",
    "  'delta_7d': 'delta_7d' in train_feat.columns\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcd9dc01-00ea-4833-98cc-cf8e7aaef0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[best.json 없음] 기본 세팅 사용\n",
      "▶ PATHS\n",
      "  train : ./data/merged_train.csv\n",
      "  test  : ./data/merged_test.csv\n",
      "  sample: ./data/sample_submission.csv\n",
      "  out   : ./artifacts/submission.csv\n",
      "▶ VAL : 2024-08-17 00:00:00 ~ 2024-08-24 23:00:00\n",
      "▶ LGB : {'objective': 'regression', 'metric': 'rmse', 'learning_rate': 0.05, 'num_leaves': 40, 'feature_fraction': 0.85, 'bagging_fraction': 0.85, 'bagging_freq': 2, 'min_data_in_leaf': 80, 'lambda_l2': 2.0, 'seed': 42, 'verbosity': -1, 'num_threads': 4}  | num_boost: 2993\n",
      "▶ CAL : AB {'a': 1.006912374168081, 'b': 3.948418827927597}\n",
      "▶ TOGGLES | std24: True | cyclic: False | slot_profile: False (rolls=4) | area_norm: False | winsor_p: None\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Config — 변수/세팅 한 곳에서 관리\n",
    "# =========================\n",
    "import os, json, numpy as np, pandas as pd\n",
    "\n",
    "# --- 경로 (상대 경로 권장) ---\n",
    "DATA_DIR       = \"./data\"\n",
    "ARTIFACTS_DIR  = \"./artifacts\"\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_MERGED_PATH = f\"{DATA_DIR}/merged_train.csv\"   # train+building 병합/전처리본\n",
    "TEST_MERGED_PATH  = f\"{DATA_DIR}/merged_test.csv\"    # test+building 병합/전처리본\n",
    "SAMPLE_SUB        = f\"{DATA_DIR}/sample_submission.csv\"\n",
    "BEST_JSON_PATH    = f\"{ARTIFACTS_DIR}/best.json\"     # 있으면 불러서 오버라이드\n",
    "OUT_SUB_PATH      = f\"{ARTIFACTS_DIR}/submission.csv\"\n",
    "\n",
    "# --- 검증 윈도우(시계열 홀드아웃) ---\n",
    "VAL_START = pd.Timestamp(2024, 8, 17, 0)\n",
    "VAL_END   = pd.Timestamp(2024, 8, 24, 23)\n",
    "\n",
    "# --- 공휴일(한국, 대회 구간) ---\n",
    "KR_HOLIDAYS = {pd.Timestamp(2024,6,6), pd.Timestamp(2024,8,15)}\n",
    "\n",
    "# --- 피처 토글(원하면 바꿔) ---\n",
    "USE_STD24          = True   # 최근 24h 표준편차\n",
    "USE_CYCLIC         = False  # hour/dow/dayofyear sin/cos\n",
    "USE_SLOT_PROFILE   = False  # (건물×요일×시간) 과거 평균\n",
    "SLOT_PROFILE_ROLLS = 4      # 위 옵션 True일 때, 과거 4회 평균(≈4주)\n",
    "USE_AREA_NORM      = False  # m² 정규화 파생\n",
    "WINSOR_P99         = None   # 예: 0.995 로 설정하면 윈저라이즈\n",
    "\n",
    "# --- 베이스라인 가중(혼합 평균) ---\n",
    "BASE_W7  = 0.70   # same-hour 7d mean\n",
    "BASE_W24 = 0.30   # last 24h mean\n",
    "\n",
    "# --- 제외 컬럼 / 카테고리 지정 ---\n",
    "EXCLUDE_COLS = {'전력소비량(kWh)', 'dt', '날짜', '시간', '일시', 'num_date_time'}\n",
    "CAT_COLS_DEFAULT = ['건물번호', '건물유형']  # factor 처리 대상\n",
    "\n",
    "# --- 하드 빌딩 가중치(난이도 높은 빌딩) ---\n",
    "HARD_BLD = {64, 1, 34, 3, 6, 10, 45, 79, 54, 23}\n",
    "HARD_BLD_WEIGHT = 1.8\n",
    "\n",
    "# --- Day1 베스트 하이퍼파라미터 & 보정(없으면 기본값) ---\n",
    "LGBM_PARAMS = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 40,\n",
    "    'feature_fraction': 0.85,\n",
    "    'bagging_fraction': 0.85,\n",
    "    'bagging_freq': 2,\n",
    "    'min_data_in_leaf': 80,\n",
    "    'lambda_l2': 2.0,\n",
    "    'seed': 42,\n",
    "    'verbosity': -1,\n",
    "    'num_threads': 4,\n",
    "}\n",
    "BEST_NUM_BOOST = 2993\n",
    "\n",
    "CALIB_MODE   = \"AB\"  # \"AB\" 또는 \"ISO\" (또는 \"ISO+BLD\")\n",
    "CALIB_PARAMS = {'a': 1.006912374168081, 'b': 3.948418827927597}\n",
    "\n",
    "# --- 랜덤서치 공간(다시 돌릴 때 사용) ---\n",
    "HP_SEARCH_SPACE = {\n",
    "    \"learning_rate\":   [0.03, 0.04, 0.05, 0.06, 0.07],\n",
    "    \"num_leaves\":      list(range(24, 96, 8)),        # 24,32,...,88\n",
    "    \"min_data_in_leaf\":[80, 100, 120, 150, 180, 220],\n",
    "    \"feature_fraction\":[0.70, 0.75, 0.80, 0.85, 0.90],\n",
    "    \"bagging_fraction\":[0.70, 0.75, 0.80, 0.85, 0.90],\n",
    "    \"bagging_freq\":    [1, 2],\n",
    "    \"lambda_l2\":       [0.0, 1.0, 2.0, 3.0, 5.0],\n",
    "}\n",
    "HP_TRIALS = 40  # 탐색 시 시도 횟수\n",
    "\n",
    "# --- best.json 있으면 우선 적용(재현성 확보) ---\n",
    "def _override_from_best_json(path: str):\n",
    "    global LGBM_PARAMS, BEST_NUM_BOOST, CALIB_MODE, CALIB_PARAMS\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            best = json.load(f)\n",
    "        if 'params' in best:       LGBM_PARAMS = {k: (float(v) if isinstance(v, (int, float)) else v)\n",
    "                                                  for k, v in best['params'].items()}\n",
    "        if 'num_boost' in best:    BEST_NUM_BOOST = int(best['num_boost'])\n",
    "        if 'calib_mode' in best:   CALIB_MODE = best['calib_mode']\n",
    "        if 'calib_params' in best: CALIB_PARAMS = {k: float(v) for k, v in best['calib_params'].items()}\n",
    "        print(f\"[best.json 적용] mode={CALIB_MODE}, num_boost={BEST_NUM_BOOST}\")\n",
    "    else:\n",
    "        print(\"[best.json 없음] 기본 세팅 사용\")\n",
    "\n",
    "_override_from_best_json(BEST_JSON_PATH)\n",
    "\n",
    "# --- 환경 출력(확인용) ---\n",
    "print(\"▶ PATHS\")\n",
    "print(\"  train :\", TRAIN_MERGED_PATH)\n",
    "print(\"  test  :\", TEST_MERGED_PATH)\n",
    "print(\"  sample:\", SAMPLE_SUB)\n",
    "print(\"  out   :\", OUT_SUB_PATH)\n",
    "print(\"▶ VAL :\", VAL_START, \"~\", VAL_END)\n",
    "print(\"▶ LGB :\", LGBM_PARAMS, \" | num_boost:\", BEST_NUM_BOOST)\n",
    "print(\"▶ CAL :\", CALIB_MODE, CALIB_PARAMS)\n",
    "print(\"▶ TOGGLES | std24:\", USE_STD24, \"| cyclic:\", USE_CYCLIC,\n",
    "      \"| slot_profile:\", USE_SLOT_PROFILE, f\"(rolls={SLOT_PROFILE_ROLLS})\",\n",
    "      \"| area_norm:\", USE_AREA_NORM, \"| winsor_p:\", WINSOR_P99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b90a0491-9d97-4b9e-8326-6dcf5d790bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def add_seasonality_features(df: pd.DataFrame,\n",
    "                             use_cyclic: bool = True,\n",
    "                             use_slot_profile: bool = False,\n",
    "                             slot_rolls: int = 4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    시계열 주기성(일중/주간/월간)을 명시적으로 피처화.\n",
    "    - use_cyclic=True: 시간/요일/월/연중 사이클릭(sin/cos)\n",
    "    - use_slot_profile=True: (건물×요일×시간) 과거 프로파일 평균(shift로 과거만 사용)\n",
    "    - slot_rolls: 프로파일 평균의 롤링 개수(주 단위 4면 최근 4주 정도)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- 기본 시간 파생 (있으면 재사용) ---\n",
    "    if 'dt' not in df.columns:\n",
    "        # '날짜','시간'에서 만들거나 '일시' 파싱 후 ensure_datetime_cols()로 채워둬야 함\n",
    "        raise ValueError(\"dt 컬럼이 필요합니다. ensure_datetime_cols() 이후 호출하세요.\")\n",
    "    if 'hour' not in df.columns:\n",
    "        df['hour'] = df['dt'].dt.hour\n",
    "    if 'weekday' not in df.columns:\n",
    "        df['weekday'] = df['dt'].dt.weekday  # 0=월, 6=일\n",
    "    if 'month' not in df.columns:\n",
    "        df['month'] = df['dt'].dt.month\n",
    "    if 'dayofyear' not in df.columns:\n",
    "        df['dayofyear'] = df['dt'].dt.dayofyear\n",
    "\n",
    "    # --- 달력/플래그 ---\n",
    "    df['is_weekend']    = (df['weekday'] >= 5).astype(int)\n",
    "    df['is_month_start'] = df['dt'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end']   = df['dt'].dt.is_month_end.astype(int)\n",
    "    df['week_of_month']  = ((df['dt'].dt.day - 1) // 7 + 1).astype(int)  # 1~5\n",
    "\n",
    "    # --- 사이클릭 인코딩(주기 신호를 부드럽게) ---\n",
    "    if use_cyclic:\n",
    "        # ① 일중(24시간)\n",
    "        df['hour_sin'] = np.sin(2*np.pi*df['hour'] / 24.0)\n",
    "        df['hour_cos'] = np.cos(2*np.pi*df['hour'] / 24.0)\n",
    "\n",
    "        # ② 주간(요일) — 요일만\n",
    "        df['dow_sin'] = np.sin(2*np.pi*df['weekday'] / 7.0)\n",
    "        df['dow_cos'] = np.cos(2*np.pi*df['weekday'] / 7.0)\n",
    "\n",
    "        # ③ 주간(시간까지): 0~167 = weekday*24 + hour\n",
    "        how = df['weekday'] * 24 + df['hour']\n",
    "        df['how_sin'] = np.sin(2*np.pi*how / 168.0)\n",
    "        df['how_cos'] = np.cos(2*np.pi*how / 168.0)\n",
    "\n",
    "        # ④ 월간(12개월)\n",
    "        df['mon_sin'] = np.sin(2*np.pi*(df['month']-1) / 12.0)\n",
    "        df['mon_cos'] = np.cos(2*np.pi*(df['month']-1) / 12.0)\n",
    "\n",
    "        # ⑤ 연중(365일) — 여름 데이터라도 부드러운 계절성 힌트\n",
    "        df['doy_sin'] = np.sin(2*np.pi*df['dayofyear'] / 366.0)\n",
    "        df['doy_cos'] = np.cos(2*np.pi*df['dayofyear'] / 366.0)\n",
    "\n",
    "    # --- (선택) 슬롯 프로파일: 건물×요일×시간 과거 평균 ---\n",
    "    if use_slot_profile:\n",
    "        # 정렬 + 그룹 정의\n",
    "        sort_cols = ['건물번호', 'dt'] if '건물번호' in df.columns else ['dt']\n",
    "        df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "        if '전력소비량(kWh)' in df.columns:\n",
    "            grp = df.groupby(['건물번호', 'weekday', 'hour'])['전력소비량(kWh)'] \\\n",
    "                    if '건물번호' in df.columns else \\\n",
    "                    df.groupby(['weekday', 'hour'])['전력소비량(kWh)']\n",
    "\n",
    "            # 과거만 보도록 shift(1) 후 최근 slot_rolls회 평균\n",
    "            df['prof_wd_h_mean'] = grp.transform(\n",
    "                lambda s: s.shift(1).rolling(slot_rolls, min_periods=1).mean()\n",
    "            )\n",
    "        else:\n",
    "            # test 구간 등 타깃이 없으면 결측 생길 수 있으니 0으로\n",
    "            df['prof_wd_h_mean'] = 0.0\n",
    "\n",
    "        # 안전 채움\n",
    "        df['prof_wd_h_mean'] = df['prof_wd_h_mean'].fillna(0.0)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97272426-b97d-489c-bb00-769732d05633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abf02980-236a-4b75-a30c-a4c3ebbc07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 0) 설정 (경로만 네 환경에 맞게 바꿔)\n",
    "# =========================================\n",
    "import os, numpy as np, pandas as pd, lightgbm as lgb\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# ▶ 필요 시 네 경로로 교체\n",
    "TRAIN_MERGED_PATH = r\"C:\\Users\\user\\Downloads\\open (1)\\merged_train.csv\"\n",
    "TEST_MERGED_PATH  = r\"C:\\Users\\user\\Downloads\\open (1)\\merged_test.csv\"      # test+building 병합본\n",
    "SAMPLE_SUB        = r\"C:\\Users\\user\\Downloads\\open (1)\\sample_submission.csv\"\n",
    "OUT_SUB           = r\"C:\\Users\\user\\Downloads\\open (1)\\submission_rmse_residual.csv\"\n",
    "os.makedirs(os.path.dirname(OUT_SUB), exist_ok=True)\n",
    "\n",
    "VAL_START = pd.Timestamp(2024,8,17,0)   # 시계열 홀드아웃\n",
    "VAL_END   = pd.Timestamp(2024,8,24,23)\n",
    "\n",
    "# (어제 쓰던 Day1 안전 파라미터)\n",
    "LGBM_PARAMS = {\n",
    "    'objective': 'regression', 'metric': 'rmse', 'learning_rate': 0.05,\n",
    "    'num_leaves': 48, 'feature_fraction': 0.85, 'bagging_fraction': 0.85,\n",
    "    'bagging_freq': 1, 'min_data_in_leaf': 150, 'lambda_l2': 3.0,\n",
    "    'seed': 42, 'verbosity': -1, 'num_threads': 4\n",
    "}\n",
    "\n",
    "# 하드 빌딩 가중(있으면 조금 더 학습)\n",
    "HARD_BLD = {64, 1, 34, 3, 6, 10, 45, 79, 54, 23}\n",
    "HARD_BLD_WEIGHT = 1.8\n",
    "\n",
    "BASE_W7, BASE_W24 = 0.70, 0.30  # baseline 혼합 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bee5f242-99a8-4c0a-af60-b4aab88d8dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 1) 유틸\n",
    "# =========================================\n",
    "def read_csv_smart(path):\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding='utf-8-sig')\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding='cp949')\n",
    "\n",
    "def ensure_datetime_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if '날짜' not in df.columns or '시간' not in df.columns:\n",
    "        if '일시' not in df.columns:\n",
    "            raise ValueError(\"일시 또는 (날짜,시간) 필요\")\n",
    "        s = df['일시'].astype(str)\n",
    "        df['날짜'] = pd.to_datetime(s.str.slice(0,8), format='%Y%m%d')\n",
    "        df['시간'] = s.str.slice(9,11).astype(int)\n",
    "    else:\n",
    "        df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "        df['시간'] = df['시간'].astype(int)\n",
    "    df['dt'] = df['날짜'] + pd.to_timedelta(df['시간'], unit='h')\n",
    "    return df\n",
    "\n",
    "def clean_capacity_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in ['태양광용량(kW)','ESS저장용량(kWh)','PCS용량(kW)']:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].replace('-', 0).astype(float)\n",
    "    return df\n",
    "\n",
    "def add_safe_weather_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if '일사(MJ/m2)' not in df.columns: df['일사(MJ/m2)'] = 0.0\n",
    "    if '일조(hr)'   not in df.columns: df['일조(hr)']   = 0.0\n",
    "    return df\n",
    "\n",
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = add_safe_weather_cols(df)\n",
    "    df = df.sort_values(['건물번호','dt']).reset_index(drop=True)\n",
    "    grp = df.groupby('건물번호', sort=False)\n",
    "\n",
    "    # 기본 라그/롤링 (과거만)\n",
    "    if '전력소비량(kWh)' in df.columns:\n",
    "        df['cons_lag1']     = grp['전력소비량(kWh)'].shift(1)\n",
    "        df['cons_lag_24h']  = grp['전력소비량(kWh)'].shift(24)\n",
    "        df['cons_lag_48h']  = grp['전력소비량(kWh)'].shift(48)\n",
    "        df['cons_lag_72h']  = grp['전력소비량(kWh)'].shift(72)\n",
    "        df['cons_lag_168h'] = grp['전력소비량(kWh)'].shift(168)\n",
    "        df['cons_mean_24h'] = grp['전력소비량(kWh)'].transform(lambda s: s.shift(1).rolling(24, min_periods=1).mean())\n",
    "        df['cons_samehour_mean_7d'] = grp['전력소비량(kWh)'].transform(lambda s: s.shift(24).rolling(7, min_periods=1).mean())\n",
    "        df['cons_std_24h'] = grp['cons_lag1'].transform(lambda s: s.rolling(24, min_periods=6).std())\n",
    "    else:\n",
    "        for c in ['cons_lag1','cons_lag_24h','cons_lag_48h','cons_lag_72h','cons_lag_168h',\n",
    "                  'cons_mean_24h','cons_samehour_mean_7d','cons_std_24h']:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    # 냉방 수요 지표\n",
    "    base_temp = 24.0\n",
    "    df['CDD'] = (df['기온(°C)'] - base_temp).clip(lower=0)\n",
    "    q99 = df['일사(MJ/m2)'].quantile(0.99) + 1e-6\n",
    "    rad_norm = (df['일사(MJ/m2)'].clip(upper=q99) / q99)\n",
    "    df['CDD_x_rad'] = df['CDD'] * rad_norm\n",
    "    df['CDD_humid_adj'] = df['CDD'] * (1 + 0.3 * (df['습도(%)'] / 100.0))\n",
    "\n",
    "    # 달력/시간\n",
    "    df['weekday']    = df['dt'].dt.weekday\n",
    "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
    "    kr_holidays = {pd.Timestamp(2024,6,6), pd.Timestamp(2024,8,15)}\n",
    "    df['is_holiday'] = df['날짜'].isin(kr_holidays).astype(int)\n",
    "    df['month']      = df['dt'].dt.month\n",
    "    df['hour']       = df['시간']\n",
    "    df['dayofyear']  = df['dt'].dt.dayofyear\n",
    "\n",
    "    # 설비·운영 힌트\n",
    "    for c in ['태양광용량(kW)','ESS저장용량(kWh)','PCS용량(kW)']:\n",
    "        if c in df.columns: df[f'log1p_{c}'] = np.log1p(df[c])\n",
    "    df['has_pv']  = (df.get('태양광용량(kW)',0)  > 0).astype(int)\n",
    "    df['has_ess'] = (df.get('ESS저장용량(kWh)',0) > 0).astype(int)\n",
    "    df['has_pcs'] = (df.get('PCS용량(kW)',0)    > 0).astype(int)\n",
    "    df['is_offpeak'] = df['hour'].isin([0,1,2,3,4,5,6,23]).astype(int)\n",
    "    df['is_peak']    = df['hour'].isin([13,14,15,16,17]).astype(int)\n",
    "    df['ess_charge_potential']    = ((df['has_ess']==1) & (df['is_offpeak']==1)).astype(int)\n",
    "    df['ess_discharge_potential'] = ((df['has_ess']==1) & (df['is_peak']==1)).astype(int)\n",
    "    df['ess_to_load_lag_ratio'] = np.where(df['cons_lag_24h'].notna(),\n",
    "                                           df.get('ESS저장용량(kWh)',0.0) / (df['cons_lag_24h'] + 1e-6),\n",
    "                                           np.nan)\n",
    "    return df\n",
    "\n",
    "def build_baseline(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"same-hour 7d mean과 24h mean 혼합. 결측은 건물 평균으로 백업\"\"\"\n",
    "    s7  = df['cons_samehour_mean_7d']\n",
    "    s24 = df['cons_mean_24h']\n",
    "    base = BASE_W7 * s7 + BASE_W24 * s24\n",
    "    # 백업: 건물별 과거 평균\n",
    "    bld_mean = df.groupby('건물번호')['전력소비량(kWh)'].transform('mean') if '전력소비량(kWh)' in df.columns else 0.0\n",
    "    return base.fillna(bld_mean).fillna(0.0)\n",
    "\n",
    "def align_for_lgb(df_tr, df_va_or_te, feature_list, cat_cols):\n",
    "    X1 = df_tr.reindex(columns=feature_list).copy()\n",
    "    X2 = df_va_or_te.reindex(columns=feature_list).copy()\n",
    "    for c in (cat_cols or []):\n",
    "        X1[c] = X1[c].astype('category')\n",
    "        X2[c] = X2[c].astype('category').cat.set_categories(X1[c].cat.categories)\n",
    "    num_cols = [c for c in feature_list if c not in (cat_cols or [])]\n",
    "    X1[num_cols] = X1[num_cols].replace([np.inf,-np.inf], np.nan).astype(float).fillna(0.0)\n",
    "    X2[num_cols] = X2[num_cols].replace([np.inf,-np.inf], np.nan).astype(float).fillna(0.0)\n",
    "    return X1, X2\n",
    "\n",
    "def RMSE(y, yhat): return float(np.sqrt(mean_squared_error(y, yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1e9d915-5d64-4b61-b6e1-cb28545da019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 2) 데이터 로드 & 전처리 & 피처\n",
    "# =========================================\n",
    "train_df = read_csv_smart(TRAIN_MERGED_PATH)\n",
    "test_df  = read_csv_smart(TEST_MERGED_PATH)\n",
    "\n",
    "train_df = clean_capacity_fields(train_df)\n",
    "test_df  = clean_capacity_fields(test_df)\n",
    "\n",
    "train_df = ensure_datetime_cols(train_df)\n",
    "test_df  = ensure_datetime_cols(test_df)\n",
    "\n",
    "train_feat = make_features(train_df)\n",
    "\n",
    "# 시계열 검증 분할\n",
    "is_val = (train_feat['dt'] >= VAL_START) & (train_feat['dt'] <= VAL_END)\n",
    "train_part = train_feat[~is_val].copy()\n",
    "valid_part = train_feat[ is_val].copy()\n",
    "\n",
    "# 피처 목록/카테고리\n",
    "EXCLUDE = {'전력소비량(kWh)','dt','날짜','시간','일시','num_date_time'}\n",
    "feature_list = [c for c in train_feat.columns if c not in EXCLUDE]\n",
    "cat_cols = [c for c in ['건물번호','건물유형'] if c in feature_list]\n",
    "\n",
    "# 행렬 정렬\n",
    "X_tr, X_va = align_for_lgb(train_part, valid_part, feature_list, cat_cols)\n",
    "y_tr, y_va = train_part['전력소비량(kWh)'], valid_part['전력소비량(kWh)']\n",
    "\n",
    "# 베이스라인\n",
    "base_tr = build_baseline(train_part)\n",
    "base_va = build_baseline(valid_part)\n",
    "\n",
    "# 잔차 타깃\n",
    "ytr_resid = (y_tr - base_tr).astype(float)\n",
    "yva_resid = (y_va - base_va).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5aef231e-380b-455a-8ed4-a5b8a7424f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's rmse: 297.494\n",
      "[400]\tvalid_0's rmse: 264.726\n",
      "[600]\tvalid_0's rmse: 252.223\n",
      "[800]\tvalid_0's rmse: 246.731\n",
      "[1000]\tvalid_0's rmse: 242.843\n",
      "[1200]\tvalid_0's rmse: 240.791\n",
      "[1400]\tvalid_0's rmse: 239.183\n",
      "[1600]\tvalid_0's rmse: 238.353\n",
      "[1800]\tvalid_0's rmse: 237.522\n",
      "[2000]\tvalid_0's rmse: 236.712\n",
      "[2200]\tvalid_0's rmse: 236.503\n",
      "[2400]\tvalid_0's rmse: 236.116\n",
      "[2600]\tvalid_0's rmse: 236.146\n",
      "[2800]\tvalid_0's rmse: 235.968\n",
      "Early stopping, best iteration is:\n",
      "[2668]\tvalid_0's rmse: 235.868\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 3) 잔차 LightGBM (RMSE 기준, 조기종료)\n",
    "# =========================================\n",
    "w_tr = np.ones(len(train_part), float)\n",
    "w_tr[train_part['건물번호'].isin(HARD_BLD).values] = HARD_BLD_WEIGHT\n",
    "\n",
    "lgb_train = lgb.Dataset(X_tr, label=ytr_resid, weight=w_tr, categorical_feature=cat_cols or None)\n",
    "lgb_valid = lgb.Dataset(X_va, label=yva_resid, categorical_feature=cat_cols or None)\n",
    "\n",
    "callbacks = [lgb.early_stopping(200), lgb.log_evaluation(200)]\n",
    "model = lgb.train(LGBM_PARAMS, lgb_train, valid_sets=[lgb_valid],\n",
    "                  num_boost_round=5000, callbacks=callbacks)\n",
    "\n",
    "pred_resid_va = model.predict(X_va, num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29528960-5726-4631-9c48-455f06978b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1732]\tvalid_0's rmse: 242.881\n",
      "[LGB 01/25] RMSE=242.881 best=242.881\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3961]\tvalid_0's rmse: 238.816\n",
      "[LGB 02/25] RMSE=238.815 best=238.815\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2032]\tvalid_0's rmse: 238.345\n",
      "[LGB 03/25] RMSE=238.345 best=238.345\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1697]\tvalid_0's rmse: 237.625\n",
      "[LGB 04/25] RMSE=237.625 best=237.625\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1969]\tvalid_0's rmse: 235.165\n",
      "[LGB 05/25] RMSE=235.165 best=235.165\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2166]\tvalid_0's rmse: 237.192\n",
      "[LGB 06/25] RMSE=237.192 best=235.165\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2463]\tvalid_0's rmse: 237.717\n",
      "[LGB 07/25] RMSE=237.717 best=235.165\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1768]\tvalid_0's rmse: 242.624\n",
      "[LGB 08/25] RMSE=242.624 best=235.165\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2074]\tvalid_0's rmse: 237.977\n",
      "[LGB 09/25] RMSE=237.977 best=235.165\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2331]\tvalid_0's rmse: 242.964\n",
      "[LGB 10/25] RMSE=242.964 best=235.165\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2071]\tvalid_0's rmse: 240.791\n",
      "[LGB 11/25] RMSE=240.791 best=235.165\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2417]\tvalid_0's rmse: 240.241\n",
      "[LGB 12/25] RMSE=240.241 best=235.165\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1496]\tvalid_0's rmse: 247.206\n",
      "[LGB 13/25] RMSE=247.206 best=235.165\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2954]\tvalid_0's rmse: 237.914\n",
      "[LGB 14/25] RMSE=237.914 best=235.165\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1479]\tvalid_0's rmse: 238.543\n",
      "[LGB 15/25] RMSE=238.543 best=235.165\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1800]\tvalid_0's rmse: 240.063\n",
      "[LGB 16/25] RMSE=240.063 best=235.165\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2695]\tvalid_0's rmse: 241.423\n",
      "[LGB 17/25] RMSE=241.423 best=235.165\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2756]\tvalid_0's rmse: 231.156\n",
      "[LGB 18/25] RMSE=231.156 best=231.156\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1988]\tvalid_0's rmse: 237.22\n",
      "[LGB 19/25] RMSE=237.220 best=231.156\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2985]\tvalid_0's rmse: 235.095\n",
      "[LGB 20/25] RMSE=235.095 best=231.156\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2015]\tvalid_0's rmse: 237.742\n",
      "[LGB 21/25] RMSE=237.742 best=231.156\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2574]\tvalid_0's rmse: 235.926\n",
      "[LGB 22/25] RMSE=235.926 best=231.156\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1538]\tvalid_0's rmse: 241.562\n",
      "[LGB 23/25] RMSE=241.562 best=231.156\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2166]\tvalid_0's rmse: 237.48\n",
      "[LGB 24/25] RMSE=237.480 best=231.156\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3999]\tvalid_0's rmse: 238.971\n",
      "[LGB 25/25] RMSE=238.971 best=231.156\n",
      "LGB Best: 231.1557270889572 {'objective': 'regression', 'metric': 'rmse', 'seed': 42, 'verbosity': -1, 'num_threads': 4, 'learning_rate': 0.06, 'num_leaves': 32, 'min_data_in_leaf': 150, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'lambda_l2': 5.0} it 2756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17276\\739483018.py:129: UserWarning: XGBoost 사용 불가: XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "  warnings.warn(f\"XGBoost 사용 불가: {e}\")\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17276\\739483018.py:158: UserWarning: CatBoost 사용 불가: No module named 'catboost'\n",
      "  warnings.warn(f\"CatBoost 사용 불가: {e}\")\n",
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] 지정된 파일을 찾을 수 없습니다\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 556, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1038, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1550, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HGB 01/15] RMSE=240.167 best=240.167\n",
      "[HGB 02/15] RMSE=223.137 best=223.137\n",
      "[HGB 03/15] RMSE=238.452 best=223.137\n",
      "[HGB 04/15] RMSE=220.102 best=220.102\n",
      "[HGB 05/15] RMSE=234.524 best=220.102\n",
      "[HGB 06/15] RMSE=242.358 best=220.102\n",
      "[HGB 07/15] RMSE=234.619 best=220.102\n",
      "[HGB 08/15] RMSE=237.798 best=220.102\n",
      "[HGB 09/15] RMSE=235.658 best=220.102\n",
      "[HGB 10/15] RMSE=229.643 best=220.102\n",
      "[HGB 11/15] RMSE=235.959 best=220.102\n",
      "[HGB 12/15] RMSE=226.135 best=220.102\n",
      "[HGB 13/15] RMSE=225.868 best=220.102\n",
      "[HGB 14/15] RMSE=239.589 best=220.102\n",
      "[HGB 15/15] RMSE=226.751 best=220.102\n",
      "HGB Best: 220.10155058057993 {'learning_rate': 0.06, 'max_leaf_nodes': 31, 'l2_regularization': 0.0, 'max_iter': 3500}\n",
      "[ETR 01/12] RMSE=316.596 best=316.596\n",
      "[ETR 02/12] RMSE=287.967 best=287.967\n",
      "[ETR 03/12] RMSE=303.665 best=287.967\n",
      "[ETR 04/12] RMSE=303.461 best=287.967\n",
      "[ETR 05/12] RMSE=323.333 best=287.967\n",
      "[ETR 06/12] RMSE=303.361 best=287.967\n",
      "[ETR 07/12] RMSE=316.033 best=287.967\n",
      "[ETR 08/12] RMSE=331.147 best=287.967\n",
      "[ETR 09/12] RMSE=303.767 best=287.967\n",
      "[ETR 10/12] RMSE=324.026 best=287.967\n",
      "[ETR 11/12] RMSE=323.069 best=287.967\n",
      "[ETR 12/12] RMSE=316.389 best=287.967\n",
      "ETR Best: 287.96670089397446 {'n_estimators': 600, 'min_samples_leaf': 3, 'max_depth': None}\n",
      "RIDGE Best: 326.2623309176218\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 221\u001b[39m\n\u001b[32m    218\u001b[39m     candidates.append((name, mdl, kind, sc, params, best_it))\n\u001b[32m    220\u001b[39m add_cand(\u001b[33m\"\u001b[39m\u001b[33mLGBM\u001b[39m\u001b[33m\"\u001b[39m, lgb_best, \u001b[33m\"\u001b[39m\u001b[33mlgb\u001b[39m\u001b[33m\"\u001b[39m, lgb_best.get(\u001b[33m\"\u001b[39m\u001b[33mbest_it\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[43madd_cand\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mXGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mxgb_best\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mskl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxgb_best\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest_it\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxgb_best\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m add_cand(\u001b[33m\"\u001b[39m\u001b[33mCAT\u001b[39m\u001b[33m\"\u001b[39m,  cb_best,  \u001b[33m\"\u001b[39m\u001b[33mskl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    223\u001b[39m add_cand(\u001b[33m\"\u001b[39m\u001b[33mHGB\u001b[39m\u001b[33m\"\u001b[39m,  hgb_best, \u001b[33m\"\u001b[39m\u001b[33mskl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 217\u001b[39m, in \u001b[36madd_cand\u001b[39m\u001b[34m(name, pack, kind, best_it)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_cand\u001b[39m(name, pack, kind, best_it=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pack \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     mdl = \u001b[43mpack\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m; sc = pack[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m]; params = pack.get(\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m    218\u001b[39m     candidates.append((name, mdl, kind, sc, params, best_it))\n",
      "\u001b[31mKeyError\u001b[39m: 'model'"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 3.5 여러 모델 랜덤서치 + 설명력 비교 + 베스트 모델 선택\n",
    "#  - 공통 타깃: residual = y - baseline\n",
    "#  - 비교 모델: LightGBM, XGBoost, CatBoost, HistGB, ExtraTrees, Ridge\n",
    "#  - 평가 지표: RMSE(정렬 키), MAE, SMAPE\n",
    "#  - 설명력: permutation importance (최종 y 기준)\n",
    "# =========================================\n",
    "import numpy as np, pandas as pd, warnings, time, random\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "# ---------- 준비물 체크 ----------\n",
    "needed = ['X_tr','X_va','y_tr','y_va','base_tr','base_va','cat_cols']\n",
    "missing = [n for n in needed if n not in globals()]\n",
    "assert not missing, f\"필요 변수 부족: {missing} 셀이 먼저 실행돼야 해.\"\n",
    "\n",
    "# ---------- 유틸 ----------\n",
    "def SMAPE(y, yhat, eps=1e-6):\n",
    "    y = np.asarray(y, float); yhat = np.asarray(yhat, float)\n",
    "    return 100.0 * np.mean(2.0 * np.abs(yhat - y) / (np.abs(y) + np.abs(yhat) + eps))\n",
    "\n",
    "def RMSE(y, yhat): return float(np.sqrt(mean_squared_error(y, yhat)))\n",
    "\n",
    "def to_sklearn_matrix(X_tr: pd.DataFrame, X_va: pd.DataFrame, cat_cols):\n",
    "    \"\"\"sklearn/XGB용: 카테고리→codes, 수치 NaN/inf 방어\"\"\"\n",
    "    Xtr = X_tr.copy(); Xva = X_va.copy()\n",
    "    use_cats = [c for c in (cat_cols or []) if c in Xtr.columns]\n",
    "    for c in use_cats:\n",
    "        Xtr[c] = Xtr[c].astype('category')\n",
    "        Xva[c] = Xva[c].astype('category').cat.set_categories(Xtr[c].cat.categories)\n",
    "        Xtr[c] = Xtr[c].cat.codes.replace(-1, np.nan)\n",
    "        Xva[c] = Xva[c].cat.codes.replace(-1, np.nan)\n",
    "    Xtr = Xtr.replace([np.inf,-np.inf], np.nan).astype(float).fillna(0.0)\n",
    "    Xva = Xva.replace([np.inf,-np.inf], np.nan).astype(float).fillna(0.0)\n",
    "    return Xtr, Xva\n",
    "\n",
    "class ResidEstimatorWrapper:\n",
    "    \"\"\"잔차 예측 모델을 최종 y 예측 모델처럼 보이게 랩핑 (permutation_importance용)\"\"\"\n",
    "    def __init__(self, fitted, base_vector, model_kind=\"skl\", lgb_best_it=None):\n",
    "        self.fitted = fitted\n",
    "        self.base = np.asarray(base_vector, float)\n",
    "        self.kind = model_kind\n",
    "        self.best_iter = lgb_best_it\n",
    "    def fit(self, X, y): return self\n",
    "    def predict(self, X):\n",
    "        if self.kind == \"lgb\":\n",
    "            r = self.fitted.predict(X, num_iteration=self.best_iter)\n",
    "        else:\n",
    "            r = self.fitted.predict(X)\n",
    "        yhat = self.base[:len(r)] + r\n",
    "        return np.clip(yhat, 0, None)\n",
    "\n",
    "# ---------- 잔차 타깃 ----------\n",
    "ytr_resid = (y_tr - base_tr).astype(float)\n",
    "yva_resid = (y_va - base_va).astype(float)\n",
    "\n",
    "# ---------- 행렬 준비 ----------\n",
    "X_tr_skl, X_va_skl = to_sklearn_matrix(X_tr, X_va, cat_cols)  # sklearn/XGB용\n",
    "X_tr_cb  = X_tr.copy(); X_va_cb = X_va.copy()                 # CatBoost용 (카테고리 유지)\n",
    "cat_idx_cb = [X_tr_cb.columns.get_loc(c) for c in cat_cols if c in X_tr_cb.columns]\n",
    "\n",
    "# ---------- (선택) 하드 빌딩 가중치 ----------\n",
    "w_tr = np.ones(len(X_tr), float)\n",
    "if 'HARD_BLD' in globals():\n",
    "    w_tr[X_tr['건물번호'].astype(str).isin(set(map(str, HARD_BLD))) ] = globals().get('HARD_BLD_WEIGHT', 1.8)\n",
    "\n",
    "# ---------- 1) LightGBM 랜덤서치 ----------\n",
    "import lightgbm as lgb\n",
    "lgb_space = {\n",
    "    \"learning_rate\":  [0.03, 0.04, 0.05, 0.06, 0.07],\n",
    "    \"num_leaves\":     list(range(32, 96, 8)),\n",
    "    \"min_data_in_leaf\":[60, 80, 100, 120, 150, 180],\n",
    "    \"feature_fraction\":[0.75, 0.80, 0.85, 0.90],\n",
    "    \"bagging_fraction\":[0.75, 0.80, 0.85, 0.90],\n",
    "    \"bagging_freq\":   [1, 2],\n",
    "    \"lambda_l2\":      [0.0, 1.0, 2.0, 3.0, 5.0],\n",
    "}\n",
    "LGB_TRIALS = 25  # 필요시 늘려\n",
    "lgb_best = {\"score\": 1e18}\n",
    "for t in range(1, LGB_TRIALS+1):\n",
    "    params = {\n",
    "        'objective':'regression','metric':'rmse','seed':42,'verbosity':-1,'num_threads':4,\n",
    "        **{k: random.choice(v) for k,v in lgb_space.items()}\n",
    "    }\n",
    "    lgb_train = lgb.Dataset(X_tr, label=ytr_resid, weight=w_tr, categorical_feature=cat_cols or None)\n",
    "    lgb_valid = lgb.Dataset(X_va, label=yva_resid, categorical_feature=cat_cols or None)\n",
    "    mdl = lgb.train(params, lgb_train, valid_sets=[lgb_valid],\n",
    "                    num_boost_round=4000, callbacks=[lgb.early_stopping(200), lgb.log_evaluation(0)])\n",
    "    pred_resid = mdl.predict(X_va, num_iteration=mdl.best_iteration)\n",
    "    yhat = np.clip(base_va.values + pred_resid, 0, None)\n",
    "    sc = RMSE(y_va, yhat)\n",
    "    if sc < lgb_best[\"score\"]:\n",
    "        lgb_best.update(dict(score=sc, model=mdl, params=params, best_it=int(mdl.best_iteration)))\n",
    "    print(f\"[LGB {t:02d}/{LGB_TRIALS}] RMSE={sc:.3f} best={lgb_best['score']:.3f}\")\n",
    "print(\"LGB Best:\", lgb_best[\"score\"], lgb_best[\"params\"], \"it\", lgb_best[\"best_it\"])\n",
    "\n",
    "# ---------- 2) XGBoost 랜덤서치 ----------\n",
    "xgb_best = None\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    xgb_space = {\n",
    "        \"learning_rate\":  [0.03, 0.04, 0.05, 0.06],\n",
    "        \"max_depth\":      [6, 7, 8, 9, 10],\n",
    "        \"min_child_weight\":[50, 80, 120, 180, 240],\n",
    "        \"subsample\":      [0.75, 0.8, 0.85, 0.9],\n",
    "        \"colsample_bytree\":[0.75, 0.8, 0.85, 0.9],\n",
    "        \"reg_lambda\":     [1.0, 2.0, 3.0, 5.0],\n",
    "        \"n_estimators\":   [2500, 3000, 3500, 4000]\n",
    "    }\n",
    "    XGB_TRIALS = 20\n",
    "    xgb_best = {\"score\": 1e18}\n",
    "    for t in range(1, XGB_TRIALS+1):\n",
    "        hp = {k: random.choice(v) for k,v in xgb_space.items()}\n",
    "        xgb = XGBRegressor(tree_method=\"hist\", random_state=42, **hp)\n",
    "        xgb.fit(X_tr_skl, ytr_resid, eval_set=[(X_va_skl, yva_resid.values)],\n",
    "                early_stopping_rounds=200, verbose=False)\n",
    "        pred_resid = xgb.predict(X_va_skl, iteration_range=(0, xgb.best_iteration))\n",
    "        yhat = np.clip(base_va.values + pred_resid, 0, None)\n",
    "        sc = RMSE(y_va, yhat)\n",
    "        if sc < xgb_best[\"score\"]:\n",
    "            xgb_best.update(dict(score=sc, model=xgb, params=hp, best_it=int(xgb.best_iteration)))\n",
    "        print(f\"[XGB {t:02d}/{XGB_TRIALS}] RMSE={sc:.3f} best={xgb_best['score']:.3f}\")\n",
    "    print(\"XGB Best:\", xgb_best[\"score\"], xgb_best[\"params\"], \"it\", xgb_best[\"best_it\"])\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"XGBoost 사용 불가: {e}\")\n",
    "\n",
    "# ---------- 3) CatBoost 랜덤서치 ----------\n",
    "cb_best = None\n",
    "try:\n",
    "    from catboost import CatBoostRegressor, Pool\n",
    "    cb_space = {\n",
    "        \"depth\":         [6, 7, 8, 9, 10],\n",
    "        \"learning_rate\": [0.03, 0.04, 0.05, 0.06],\n",
    "        \"l2_leaf_reg\":   [1.0, 2.0, 3.0, 5.0],\n",
    "        \"bagging_temperature\": [0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "        \"iterations\":    [3000, 4000, 5000]\n",
    "    }\n",
    "    CB_TRIALS = 20\n",
    "    train_pool = Pool(X_tr_cb, label=ytr_resid, cat_features=cat_idx_cb)\n",
    "    valid_pool = Pool(X_va_cb, label=yva_resid, cat_features=cat_idx_cb)\n",
    "    cb_best = {\"score\": 1e18}\n",
    "    for t in range(1, CB_TRIALS+1):\n",
    "        hp = {k: random.choice(v) for k,v in cb_space.items()}\n",
    "        cb = CatBoostRegressor(loss_function=\"RMSE\", random_seed=42, **hp, verbose=False)\n",
    "        cb.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=200, verbose=False)\n",
    "        pred_resid = cb.predict(valid_pool)\n",
    "        yhat = np.clip(base_va.values + pred_resid, 0, None)\n",
    "        sc = RMSE(y_va, yhat)\n",
    "        if sc < cb_best[\"score\"]:\n",
    "            cb_best.update(dict(score=sc, model=cb, params=hp))\n",
    "        print(f\"[CAT {t:02d}/{CB_TRIALS}] RMSE={sc:.3f} best={cb_best['score']:.3f}\")\n",
    "    print(\"CAT Best:\", cb_best[\"score\"], cb_best[\"params\"])\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"CatBoost 사용 불가: {e}\")\n",
    "\n",
    "# ---------- 4) HistGradientBoosting 랜덤서치(소형) ----------\n",
    "hgb_space = {\n",
    "    \"learning_rate\":    [0.03, 0.04, 0.05, 0.06],\n",
    "    \"max_leaf_nodes\":   [31, 47, 63, 79],\n",
    "    \"l2_regularization\":[0.0, 0.05, 0.1, 0.2],\n",
    "    \"max_iter\":         [1500, 2500, 3500]\n",
    "}\n",
    "HGB_TRIALS = 15\n",
    "hgb_best = {\"score\": 1e18}\n",
    "for t in range(1, HGB_TRIALS+1):\n",
    "    hp = {k: random.choice(v) for k,v in hgb_space.items()}\n",
    "    hgb = HistGradientBoostingRegressor(\n",
    "        early_stopping=True, validation_fraction=0.1, n_iter_no_change=100, random_state=42, **hp\n",
    "    )\n",
    "    hgb.fit(X_tr_skl, ytr_resid)\n",
    "    pred_resid = hgb.predict(X_va_skl)\n",
    "    yhat = np.clip(base_va.values + pred_resid, 0, None)\n",
    "    sc = RMSE(y_va, yhat)\n",
    "    if sc < hgb_best[\"score\"]:\n",
    "        hgb_best.update(dict(score=sc, model=hgb, params=hp))\n",
    "    print(f\"[HGB {t:02d}/{HGB_TRIALS}] RMSE={sc:.3f} best={hgb_best['score']:.3f}\")\n",
    "print(\"HGB Best:\", hgb_best[\"score\"], hgb_best[\"params\"])\n",
    "\n",
    "# ---------- 5) ExtraTrees 랜덤서치(소형) + Ridge 고정 그리드 ----------\n",
    "etr_space = {\n",
    "    \"n_estimators\":     [600, 800, 1000],\n",
    "    \"min_samples_leaf\": [3, 5, 8],\n",
    "    \"max_depth\":        [None, 18, 24]\n",
    "}\n",
    "ETR_TRIALS = 12\n",
    "etr_best = {\"score\": 1e18}\n",
    "for t in range(1, ETR_TRIALS+1):\n",
    "    hp = {k: random.choice(v) for k,v in etr_space.items()}\n",
    "    etr = ExtraTreesRegressor(n_jobs=-1, random_state=42, **hp)\n",
    "    etr.fit(X_tr_skl, ytr_resid)\n",
    "    pred_resid = etr.predict(X_va_skl)\n",
    "    yhat = np.clip(base_va.values + pred_resid, 0, None)\n",
    "    sc = RMSE(y_va, yhat)\n",
    "    if sc < etr_best[\"score\"]:\n",
    "        etr_best.update(dict(score=sc, model=etr, params=hp))\n",
    "    print(f\"[ETR {t:02d}/{ETR_TRIALS}] RMSE={sc:.3f} best={etr_best['score']:.3f}\")\n",
    "print(\"ETR Best:\", etr_best[\"score\"], etr_best[\"params\"])\n",
    "\n",
    "ridge = make_pipeline(\n",
    "    StandardScaler(with_mean=False),\n",
    "    RidgeCV(alphas=[0.1, 0.3, 1.0, 3.0, 10.0], cv=5)\n",
    ")\n",
    "ridge.fit(X_tr_skl, ytr_resid)\n",
    "pred_resid = ridge.predict(X_va_skl)\n",
    "yhat = np.clip(base_va.values + pred_resid, 0, None)\n",
    "ridge_best = {\"score\": RMSE(y_va, yhat), \"model\": ridge, \"params\": {\"alphas\":[0.1,0.3,1,3,10]}}\n",
    "print(\"RIDGE Best:\", ridge_best[\"score\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1e435b3-92be-4b15-8d1e-ce88c00ec5b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     candidates.append((name, mdl, kind, sc, params, best_it))\n\u001b[32m      8\u001b[39m add_cand(\u001b[33m\"\u001b[39m\u001b[33mLGBM\u001b[39m\u001b[33m\"\u001b[39m, lgb_best, \u001b[33m\"\u001b[39m\u001b[33mlgb\u001b[39m\u001b[33m\"\u001b[39m, lgb_best.get(\u001b[33m\"\u001b[39m\u001b[33mbest_it\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43madd_cand\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mXGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mxgb_best\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mskl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxgb_best\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest_it\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxgb_best\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m add_cand(\u001b[33m\"\u001b[39m\u001b[33mCAT\u001b[39m\u001b[33m\"\u001b[39m,  cb_best,  \u001b[33m\"\u001b[39m\u001b[33mskl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     11\u001b[39m add_cand(\u001b[33m\"\u001b[39m\u001b[33mHGB\u001b[39m\u001b[33m\"\u001b[39m,  hgb_best, \u001b[33m\"\u001b[39m\u001b[33mskl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36madd_cand\u001b[39m\u001b[34m(name, pack, kind, best_it)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_cand\u001b[39m(name, pack, kind, best_it=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pack \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     mdl = \u001b[43mpack\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m; sc = pack[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m]; params = pack.get(\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m      6\u001b[39m     candidates.append((name, mdl, kind, sc, params, best_it))\n",
      "\u001b[31mKeyError\u001b[39m: 'model'"
     ]
    }
   ],
   "source": [
    "# ---------- 종합 비교 ----------\n",
    "candidates = []\n",
    "def add_cand(name, pack, kind, best_it=None):\n",
    "    if pack is None: return\n",
    "    mdl = pack[\"model\"]; sc = pack[\"score\"]; params = pack.get(\"params\", {})\n",
    "    candidates.append((name, mdl, kind, sc, params, best_it))\n",
    "\n",
    "add_cand(\"LGBM\", lgb_best, \"lgb\", lgb_best.get(\"best_it\"))\n",
    "add_cand(\"XGB\",  xgb_best, \"skl\", xgb_best.get(\"best_it\") if xgb_best else None)\n",
    "add_cand(\"CAT\",  cb_best,  \"skl\", None)\n",
    "add_cand(\"HGB\",  hgb_best, \"skl\", None)\n",
    "add_cand(\"ETR\",  etr_best, \"skl\", None)\n",
    "add_cand(\"RIDGE\",ridge_best,\"skl\", None)\n",
    "\n",
    "rows = []\n",
    "for name, mdl, kind, sc, params, best_it in candidates:\n",
    "    # 다시 예측(안전)\n",
    "    if kind == \"lgb\":\n",
    "        pred_resid = mdl.predict(X_va, num_iteration=best_it)\n",
    "    else:\n",
    "        pred_resid = mdl.predict(X_va_skl if name!=\"CAT\" else X_va_skl)  # CAT은 위에서 평가됨, 아래에서 재평가\n",
    "        if name==\"CAT\" and cb_best is not None:\n",
    "            from catboost import Pool\n",
    "            pred_resid = cb_best[\"model\"].predict(Pool(X_va_cb, cat_features=cat_idx_cb))\n",
    "    yhat = np.clip(base_va.values + pred_resid, 0, None)\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"RMSE\": RMSE(y_va, yhat),\n",
    "        \"MAE\": mean_absolute_error(y_va, yhat),\n",
    "        \"SMAPE\": SMAPE(y_va, yhat),\n",
    "        \"params\": params,\n",
    "        \"best_iter\": best_it\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows).sort_values(\"RMSE\").reset_index(drop=True)\n",
    "print(\"\\n[VAL 성능 비교] ↓RMSE\")\n",
    "display(results_df)\n",
    "\n",
    "# ---------- 베스트 모델 설명력(Permutation Importance) ----------\n",
    "best_row = results_df.iloc[0]\n",
    "BEST_MODEL_NAME = best_row[\"model\"]\n",
    "print(f\"\\n[베스트 모델] {BEST_MODEL_NAME}\")\n",
    "\n",
    "# 어떤 입력행렬/예측 함수를 쓸지 선택\n",
    "if BEST_MODEL_NAME == \"LGBM\":\n",
    "    best_obj = lgb_best[\"model\"]; best_kind = \"lgb\"; best_it = lgb_best[\"best_it\"]; Xv = X_va\n",
    "elif BEST_MODEL_NAME == \"XGB\":\n",
    "    best_obj = xgb_best[\"model\"]; best_kind = \"skl\"; best_it = xgb_best[\"best_it\"]; Xv = X_va_skl\n",
    "elif BEST_MODEL_NAME == \"CAT\":\n",
    "    best_obj = cb_best[\"model\"];  best_kind = \"cat\"; best_it = None; Xv = X_va_cb\n",
    "elif BEST_MODEL_NAME == \"HGB\":\n",
    "    best_obj = hgb_best[\"model\"]; best_kind = \"skl\"; best_it = None; Xv = X_va_skl\n",
    "elif BEST_MODEL_NAME == \"ETR\":\n",
    "    best_obj = etr_best[\"model\"]; best_kind = \"skl\"; best_it = None; Xv = X_va_skl\n",
    "else:\n",
    "    best_obj = ridge_best[\"model\"]; best_kind = \"skl\"; best_it = None; Xv = X_va_skl\n",
    "\n",
    "# perm importance는 래퍼로 최종 y 기준 계산\n",
    "if BEST_MODEL_NAME == \"CAT\":\n",
    "    # CatBoost는 predict가 Pool 필요 → 커스텀 predict로 감쌈\n",
    "    from catboost import Pool\n",
    "    class CatResidWrapper(ResidEstimatorWrapper):\n",
    "        def predict(self, X):\n",
    "            pool = Pool(X_va_cb, cat_features=cat_idx_cb)  # 검사시 X 무시하고 고정 VAL 사용\n",
    "            r = cb_best[\"model\"].predict(pool)\n",
    "            return np.clip(base_va.values + r, 0, None)\n",
    "    wrapper = CatResidWrapper(best_obj, base_va.reset_index(drop=True), \"skl\")\n",
    "    X_for_pi = X_va_cb  # 특성 이름 유지\n",
    "else:\n",
    "    wrapper = ResidEstimatorWrapper(best_obj, base_va.reset_index(drop=True),\n",
    "                                    \"lgb\" if BEST_MODEL_NAME==\"LGBM\" else \"skl\",\n",
    "                                    lgb_best.get(\"best_it\") if BEST_MODEL_NAME==\"LGBM\" else None)\n",
    "    X_for_pi = Xv\n",
    "\n",
    "pi = permutation_importance(\n",
    "    wrapper, X_for_pi, y_va.values, n_repeats=5, random_state=42,\n",
    "    scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "imp_df = pd.DataFrame({\n",
    "    \"feature\": getattr(X_for_pi, \"columns\", pd.Index(range(X_for_pi.shape[1]))),\n",
    "    \"imp\": pi.importances_mean,\n",
    "    \"std\": pi.importances_std\n",
    "}).sort_values(\"imp\", ascending=False)\n",
    "\n",
    "print(\"\\n[베스트 모델 설명력 Top-20]\")\n",
    "display(imp_df.head(20))\n",
    "\n",
    "# 다음 단계에서 재학습/보정/제출에 쓰라고 전역 저장\n",
    "BEST_MODEL_OBJ  = best_obj\n",
    "BEST_MODEL_KIND = best_kind\n",
    "BEST_NUM_BOOST  = (lgb_best[\"best_it\"] if BEST_MODEL_NAME==\"LGBM\" else None)\n",
    "BEST_PARAMS     = [r for r in rows if r[\"model\"]==BEST_MODEL_NAME][0][\"params\"]\n",
    "print(f\"\\n[SAVED] BEST_MODEL_NAME={BEST_MODEL_NAME}, KIND={BEST_MODEL_KIND}, BEST_NUM_BOOST={BEST_NUM_BOOST}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81a97ec8-1830-4993-8f77-6ffbc65e8dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Calibration] best=ISO  RMSE=211.611\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5R) 베스트(HGB) 보정 선택 (AB vs ISO)\n",
    "# =========================\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "def RMSE(y, yhat): \n",
    "    return float(np.sqrt(mean_squared_error(y, yhat)))\n",
    "\n",
    "assert 'hgb_best' in globals() and hgb_best.get(\"model\") is not None, \"hgb_best가 없습니다. 위 랜덤서치 셀을 먼저 실행하세요.\"\n",
    "\n",
    "# VALID에서 잔차 예측\n",
    "pred_resid_va = hgb_best[\"model\"].predict(X_va_skl)\n",
    "base_vec      = base_va.values\n",
    "yhat_raw      = np.clip(base_vec + pred_resid_va, 0, None)\n",
    "\n",
    "# AB(선형) 보정\n",
    "A = np.vstack([pred_resid_va, np.ones_like(pred_resid_va)]).T\n",
    "a, b = np.linalg.lstsq(A, (y_va - base_va).values, rcond=None)[0]\n",
    "yhat_ab  = np.clip(base_vec + (a * pred_resid_va + b), 0, None)\n",
    "\n",
    "# Isotonic 보정\n",
    "iso = IsotonicRegression(out_of_bounds='clip')\n",
    "iso.fit(pred_resid_va, (y_va - base_va).values)\n",
    "yhat_iso = np.clip(base_vec + iso.transform(pred_resid_va), 0, None)\n",
    "\n",
    "rmse_raw = RMSE(y_va, yhat_raw)\n",
    "rmse_ab  = RMSE(y_va, yhat_ab)\n",
    "rmse_iso = RMSE(y_va, yhat_iso)\n",
    "\n",
    "if rmse_iso < rmse_ab and rmse_iso < rmse_raw:\n",
    "    CALIB_MODE = \"ISO\"; CALIB_PARAMS = {\"iso\": iso}\n",
    "    print(f\"[Calibration] best=ISO  RMSE={rmse_iso:.3f}\")\n",
    "elif rmse_ab < rmse_raw:\n",
    "    CALIB_MODE = \"AB\";  CALIB_PARAMS = {\"a\": float(a), \"b\": float(b)}\n",
    "    print(f\"[Calibration] best=AB   RMSE={rmse_ab:.3f}  (a={a:.3f}, b={b:.3f})\")\n",
    "else:\n",
    "    CALIB_MODE = \"NONE\"; CALIB_PARAMS = {}\n",
    "    print(f\"[Calibration] best=NONE RMSE={rmse_raw:.3f}\")\n",
    "\n",
    "# 추후 재사용 위해 저장\n",
    "BEST_MODEL_NAME = \"HGB\"\n",
    "BEST_MODEL_OBJ  = hgb_best[\"model\"]\n",
    "BEST_MODEL_KIND = \"skl\"\n",
    "BEST_PARAMS     = hgb_best.get(\"params\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c96a0f40-9e82-4cbe-934f-04d173beaefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Check] HGB VAL RMSE=99.666\n",
      "[DONE] 결측 0 채움 버전 저장: C:\\Users\\user\\Downloads\\open (1)\\submission_rmse_residual.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 6R) 드롭 없이 결측치 0 채움 버전 (HGB 최종)\n",
    "#  - 선행: all_tr, all_te, FEATS, cat_cols_use, X_full_skl, X_te_skl 가 만들어져 있어야 함\n",
    "#  - to_sklearn_matrix 가 수치 NaN/inf → 0 처리하므로, y/베이스라인도 0으로 강제\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def RMSE(y, yhat):\n",
    "    return float(np.sqrt(mean_squared_error(y, yhat)))\n",
    "\n",
    "# 0) 안전: FEATS 안의 수치컬럼들과 타깃의 결측/inf 0 채움\n",
    "for _df in (all_tr, all_te):\n",
    "    num_cols = [c for c in FEATS if c in _df.columns and pd.api.types.is_numeric_dtype(_df[c])]\n",
    "    # 타깃 컬럼도 함께 처리(학습쪽만 존재)\n",
    "    if '전력소비량(kWh)' in _df.columns:\n",
    "        num_cols = list(dict.fromkeys(num_cols + ['전력소비량(kWh)']))\n",
    "    _df[num_cols] = (_df[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0))\n",
    "\n",
    "# 1) 베이스라인 테이블 만들기 (train 기준)\n",
    "g1 = all_tr.groupby(['건물번호','hour','weekday'])['전력소비량(kWh)'].mean()\n",
    "g2 = all_tr.groupby(['건물번호','hour'])['전력소비량(kWh)'].mean()\n",
    "g3 = all_tr.groupby(['건물번호'])['전력소비량(kWh)'].mean()\n",
    "g4 = float(all_tr['전력소비량(kWh)'].mean())  # 전체 평균\n",
    "\n",
    "# 2) 베이스라인 매핑 + 최종적으로 0 채움\n",
    "def map_baseline(df: pd.DataFrame):\n",
    "    s = pd.Series(\n",
    "        g1.reindex(df.set_index(['건물번호','hour','weekday']).index).to_numpy(),\n",
    "        index=df.index\n",
    "    )\n",
    "    s = s.fillna(pd.Series(df.set_index(['건물번호','hour']).index.map(g2), index=df.index))\n",
    "    s = s.fillna(df['건물번호'].map(g3))\n",
    "    s = s.fillna(g4)\n",
    "    return s.fillna(0.0).astype(float)\n",
    "\n",
    "base_full = map_baseline(all_tr)\n",
    "base_te   = map_baseline(all_te)\n",
    "\n",
    "# 3) 잔차 타깃 구성 (무조건 0으로 채운 상태)\n",
    "y_full_resid = (all_tr['전력소비량(kWh)'].fillna(0.0) - base_full.fillna(0.0)).astype(float)\n",
    "\n",
    "# 4) HGB 파라미터 (랜덤서치 결과가 있으면 반영)\n",
    "best_hp = {'learning_rate':0.06, 'max_leaf_nodes':31, 'l2_regularization':0.0, 'max_iter':3500}\n",
    "if 'hgb_best' in globals() and isinstance(hgb_best.get('params', {}), dict):\n",
    "    best_hp.update(hgb_best['params'])\n",
    "\n",
    "hgb_final = HistGradientBoostingRegressor(\n",
    "    early_stopping=False, random_state=42, **best_hp\n",
    ")\n",
    "\n",
    "# 5) 학습 (to_sklearn_matrix가 X_full_skl/X_te_skl 내 NaN/inf → 0 처리함)\n",
    "hgb_final.fit(X_full_skl, y_full_resid)\n",
    "\n",
    "# (옵션) 검증 체크\n",
    "if 'is_val' in globals():\n",
    "    va_idx = np.where(is_val.values)[0]\n",
    "    X_va_chk    = X_full_skl.iloc[va_idx]\n",
    "    base_va_chk = base_full.iloc[va_idx]\n",
    "    y_va_chk    = all_tr['전력소비량(kWh)'].iloc[va_idx]\n",
    "    # 혹시나 있을 결측 0 채움\n",
    "    y_va_chk = y_va_chk.fillna(0.0)\n",
    "    base_va_chk = base_va_chk.fillna(0.0)\n",
    "    pred_resid  = hgb_final.predict(X_va_chk)\n",
    "    yhat_va     = np.clip(base_va_chk.values + pred_resid, 0, None)\n",
    "    print(f\"[Check] HGB VAL RMSE={RMSE(y_va_chk, yhat_va):.3f}\")\n",
    "\n",
    "# 6) 보정 적용 (있으면), 없으면 생략\n",
    "CALIB_MODE   = globals().get('CALIB_MODE', 'NONE')\n",
    "CALIB_PARAMS = globals().get('CALIB_PARAMS', {})\n",
    "\n",
    "resid_te = hgb_final.predict(X_te_skl)\n",
    "if CALIB_MODE == \"ISO\":\n",
    "    test_pred = base_te.values + CALIB_PARAMS[\"iso\"].transform(resid_te)\n",
    "elif CALIB_MODE == \"AB\":\n",
    "    test_pred = base_te.values + CALIB_PARAMS[\"a\"] * resid_te + CALIB_PARAMS[\"b\"]\n",
    "else:\n",
    "    test_pred = base_te.values + resid_te\n",
    "\n",
    "# 7) 음수 방지 + 저장\n",
    "test_pred = np.clip(test_pred, 0, None)\n",
    "sub = read_csv_smart(SAMPLE_SUB)\n",
    "sub['answer'] = pd.Series(test_pred, index=sub.index)\n",
    "sub.to_csv(OUT_SUB, index=False, encoding='utf-8-sig')\n",
    "print(f\"[DONE] 결측 0 채움 버전 저장: {OUT_SUB}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efddb510-17fd-4e8d-a7ce-062fdd633d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in target (train): 15456\n",
      "NaN in base_full (before fix): 0\n"
     ]
    }
   ],
   "source": [
    "# 진단: 어디에 NaN이 있는지 확인\n",
    "print(\"NaN in target (train):\", int(all_tr['전력소비량(kWh)'].isna().sum()))\n",
    "try:\n",
    "    print(\"NaN in base_full (before fix):\", int(base_full.isna().sum()))\n",
    "except NameError:\n",
    "    print(\"base_full 아직 안만듦\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29ce4c80-d181-437b-af66-b657cff9daa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 138\u001b[39m\n\u001b[32m    133\u001b[39m hgb_final = HistGradientBoostingRegressor(\n\u001b[32m    134\u001b[39m     early_stopping=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=\u001b[32m42\u001b[39m, **best_hp\n\u001b[32m    135\u001b[39m )\n\u001b[32m    137\u001b[39m y_full_resid = (all_tr[\u001b[33m'\u001b[39m\u001b[33m전력소비량(kWh)\u001b[39m\u001b[33m'\u001b[39m] - base_full).astype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[43mhgb_final\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_full_skl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_full_resid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# ---- 8) VALID 성능(선택) 확인: VAL 구간 변수 있으면 점검\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mis_val\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:539\u001b[39m, in \u001b[36mBaseHistGradientBoosting.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    537\u001b[39m acc_prediction_time = \u001b[32m0.0\u001b[39m\n\u001b[32m    538\u001b[39m X, known_categories = \u001b[38;5;28mself\u001b[39m._preprocess_X(X, reset=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m y = \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m y = \u001b[38;5;28mself\u001b[39m._encode_y(y)\n\u001b[32m    541\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1409\u001b[39m, in \u001b[36m_check_y\u001b[39m\u001b[34m(y, multi_output, y_numeric, estimator)\u001b[39m\n\u001b[32m   1407\u001b[39m     estimator_name = _check_estimator_name(estimator)\n\u001b[32m   1408\u001b[39m     y = column_or_1d(y, warn=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1409\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m     _ensure_no_complex_data(y)\n\u001b[32m   1411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y.dtype, \u001b[33m\"\u001b[39m\u001b[33mkind\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m y.dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mO\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# (Fix) 6R 전에 all_tr / all_te 만들고 이어서 HGB 최종 추론\n",
    "#  - 기존 전처리/피처 함수(make_features 등)가 이미 선언돼 있다고 가정\n",
    "#  - TEST_MERGED_PATH 있으면 그대로 쓰고, 없으면 TEST_PATH+BUILD_PATH merge\n",
    "# =========================================\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- 경로 기본값 (이미 정의돼 있으면 그대로 사용) ----\n",
    "TRAIN_MERGED_PATH = globals().get(\"TRAIN_MERGED_PATH\", r\"C:\\Users\\user\\Downloads\\open (1)\\merged_train.csv\")\n",
    "TEST_MERGED_PATH  = globals().get(\"TEST_MERGED_PATH\",  r\"C:\\Users\\user\\Downloads\\open (1)\\merged_test.csv\")  # 있다고 했던 경로\n",
    "TEST_PATH         = globals().get(\"TEST_PATH\",         r\"C:\\Users\\user\\Downloads\\open (1)\\test.csv\")\n",
    "BUILD_PATH        = globals().get(\"BUILD_PATH\",        r\"C:\\Users\\user\\Downloads\\open (1)\\building_info.csv\")\n",
    "SAMPLE_SUB        = globals().get(\"SAMPLE_SUB\",        r\"C:\\Users\\user\\Downloads\\open (1)\\sample_submission.csv\")\n",
    "OUT_SUB           = globals().get(\"OUT_SUB\",           r\"C:\\Users\\user\\Downloads\\open (1)\\submission.csv\")\n",
    "\n",
    "# ---- 유틸 (존재하면 재정의 안 함) ----\n",
    "def _read_csv_smart(path, **kwargs):\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=kwargs.get('encoding', 'utf-8-sig'))\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding='cp949')\n",
    "\n",
    "if 'read_csv_smart' not in globals():\n",
    "    read_csv_smart = _read_csv_smart\n",
    "\n",
    "if 'to_sklearn_matrix' not in globals():\n",
    "    # sklearn/HGB용 행렬 변환\n",
    "    def to_sklearn_matrix(X_tr: pd.DataFrame, X_te: pd.DataFrame, cat_cols):\n",
    "        Xtr = X_tr.copy(); Xte = X_te.copy()\n",
    "        use_cats = [c for c in (cat_cols or []) if c in Xtr.columns]\n",
    "        for c in use_cats:\n",
    "            Xtr[c] = Xtr[c].astype('category')\n",
    "            Xte[c] = Xte[c].astype('category').cat.set_categories(Xtr[c].cat.categories)\n",
    "            Xtr[c] = Xtr[c].cat.codes.replace(-1, np.nan)\n",
    "            Xte[c] = Xte[c].cat.codes.replace(-1, np.nan)\n",
    "        Xtr = Xtr.replace([np.inf,-np.inf], np.nan).astype(float).fillna(0.0)\n",
    "        Xte = Xte.replace([np.inf,-np.inf], np.nan).astype(float).fillna(0.0)\n",
    "        return Xtr, Xte\n",
    "\n",
    "# ---- 필요한 함수들이 있는지 체크 ----\n",
    "needed_funcs = ['clean_capacity_fields','ensure_datetime_cols','make_features']\n",
    "missing = [f for f in needed_funcs if f not in globals()]\n",
    "assert not missing, f\"다음 전처리/피처 함수가 필요해: {missing} (앞 셀 실행 먼저)\"\n",
    "\n",
    "# ---- 1) 데이터 로드 ----\n",
    "train_df = read_csv_smart(TRAIN_MERGED_PATH)\n",
    "\n",
    "# test는 merged 우선, 없으면 test+building merge\n",
    "if Path(TEST_MERGED_PATH).exists():\n",
    "    test_df = read_csv_smart(TEST_MERGED_PATH)\n",
    "else:\n",
    "    test_raw = read_csv_smart(TEST_PATH)\n",
    "    build_df = read_csv_smart(BUILD_PATH)\n",
    "    test_df = pd.merge(test_raw, build_df, on='건물번호', how='left')\n",
    "\n",
    "# ---- 2) 동일 전처리 ----\n",
    "train_df = clean_capacity_fields(train_df)\n",
    "train_df = ensure_datetime_cols(train_df)\n",
    "\n",
    "test_df  = clean_capacity_fields(test_df)\n",
    "test_df  = ensure_datetime_cols(test_df)\n",
    "\n",
    "# 일사/일조 방어\n",
    "if '일사(MJ/m2)' not in train_df.columns: train_df['일사(MJ/m2)'] = 0.0\n",
    "if '일조(hr)'   not in train_df.columns: train_df['일조(hr)']   = 0.0\n",
    "if '일사(MJ/m2)' not in test_df.columns:  test_df['일사(MJ/m2)']  = 0.0\n",
    "if '일조(hr)'   not in test_df.columns:   test_df['일조(hr)']    = 0.0\n",
    "\n",
    "# ---- 3) train+test concat → 동일 피처 생성 ----\n",
    "all_df   = pd.concat([train_df, test_df], ignore_index=True)\n",
    "all_feat = make_features(all_df)\n",
    "\n",
    "# split back\n",
    "all_tr = all_feat.iloc[:len(train_df)].copy()\n",
    "all_te = all_feat.iloc[len(train_df):].copy()\n",
    "\n",
    "# ---- 4) 피처 목록 동기화 ----\n",
    "#   - 가능하면 기존 학습에 썼던 X_tr.columns를 그대로 사용(일관성 유지)\n",
    "#   - 없으면 FEATURE_LIST 또는 규칙 기반으로 재구성\n",
    "EXCLUDE = {'전력소비량(kWh)','dt','날짜','시간','일시','num_date_time'}\n",
    "if 'X_tr' in globals():\n",
    "    FEATS = list(X_tr.columns)\n",
    "else:\n",
    "    if 'FEATURE_LIST' in globals():\n",
    "        FEATS = [c for c in FEATURE_LIST if c not in EXCLUDE]\n",
    "    else:\n",
    "        from pandas.api.types import is_numeric_dtype\n",
    "        FEATS = [c for c in all_tr.columns if c not in EXCLUDE and (is_numeric_dtype(all_tr[c]) or str(all_tr[c].dtype)=='category')]\n",
    "\n",
    "# 카테고리(factor) 지정\n",
    "if 'cat_cols' in globals():\n",
    "    cat_cols_use = [c for c in cat_cols if c in FEATS]\n",
    "else:\n",
    "    cat_cols_use = [c for c in ['건물번호','건물유형'] if c in FEATS]\n",
    "\n",
    "# 공통 교집합만 사용(혹시라도 누락 방지)\n",
    "FEATS = [c for c in FEATS if c in all_tr.columns and c in all_te.columns]\n",
    "\n",
    "# ---- 5) 스키런 행렬 변환(HGB용) ----\n",
    "X_full_skl, X_te_skl = to_sklearn_matrix(all_tr[FEATS], all_te[FEATS], cat_cols_use)\n",
    "\n",
    "# ---- 6) 베이스라인 함수 준비 ----\n",
    "if 'build_baseline' not in globals():\n",
    "    # 간단한 베이스라인: 건물번호 × hour × weekday 평균\n",
    "    def build_baseline(df: pd.DataFrame) -> pd.Series:\n",
    "        key = ['건물번호','hour','weekday'] if all(k in df.columns for k in ['hour','weekday']) else ['건물번호','시간']\n",
    "        if '전력소비량(kWh)' in df.columns:\n",
    "            grp_mean = df.groupby(key)['전력소비량(kWh)'].transform('mean')\n",
    "            return grp_mean\n",
    "        else:\n",
    "            # test용: train 평균을 test 키에 매핑하려면 외부 map이 필요하지만\n",
    "            # 여기서는 concat 기반이라 transform로도 채워짐(앞단 all_df concat 덕분)\n",
    "            return df.groupby(key)[key[0]].transform('count')*0.0  # dummy → 아래에서 다시 채움\n",
    "\n",
    "# concat 기반이라 all_tr에서 평균을 만들고 같은 키가 all_te에도 존재 → transform 결과가 all_df 전 구간에 퍼져 있음\n",
    "base_full = all_tr.groupby(['건물번호','hour','weekday'])['전력소비량(kWh)'].transform('mean')\n",
    "# test는 훈련 평균을 map\n",
    "mean_tbl  = all_tr.groupby(['건물번호','hour','weekday'])['전력소비량(kWh)'].mean()\n",
    "base_te   = all_te.set_index(['건물번호','hour','weekday']).index.map(mean_tbl).astype(float)\n",
    "base_te   = pd.Series(base_te, index=all_te.index).fillna(all_tr['전력소비량(kWh)'].mean())\n",
    "\n",
    "# ---- 7) HGB 최종 재학습 ----\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "def RMSE(y, yhat):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return float(np.sqrt(mean_squared_error(y, yhat)))\n",
    "\n",
    "# 랜덤서치에서 얻은 베스트 파라미터를 쓰되, 없으면 기본값\n",
    "best_hp = {'learning_rate':0.06,'max_leaf_nodes':31,'l2_regularization':0.0,'max_iter':3500}\n",
    "best_hp.update(globals().get('hgb_best', {}).get('params', {}))\n",
    "\n",
    "hgb_final = HistGradientBoostingRegressor(\n",
    "    early_stopping=False, random_state=42, **best_hp\n",
    ")\n",
    "\n",
    "y_full_resid = (all_tr['전력소비량(kWh)'] - base_full).astype(float)\n",
    "hgb_final.fit(X_full_skl, y_full_resid)\n",
    "\n",
    "# ---- 8) VALID 성능(선택) 확인: VAL 구간 변수 있으면 점검\n",
    "if 'is_val' in globals():\n",
    "    val_idx = np.where(is_val.values)[0]\n",
    "    tr_idx  = np.where(~is_val.values)[0]\n",
    "    # 같은 FEATS 기준으로 스키런 행렬 다시 슬라이스\n",
    "    X_va_chk = X_full_skl.iloc[val_idx]\n",
    "    base_va_chk = base_full.iloc[val_idx]\n",
    "    y_va_chk = all_tr['전력소비량(kWh)'].iloc[val_idx]\n",
    "    pred_resid_va = hgb_final.predict(X_va_chk)\n",
    "    yhat_va = np.clip(base_va_chk.values + pred_resid_va, 0, None)\n",
    "    print(f\"[Check] HGB VAL RMSE={RMSE(y_va_chk, yhat_va):.3f}\")\n",
    "\n",
    "# ---- 9) 보정 적용(CALIB_MODE 있으면), 없으면 생략\n",
    "CALIB_MODE   = globals().get('CALIB_MODE', 'NONE')\n",
    "CALIB_PARAMS = globals().get('CALIB_PARAMS', {})\n",
    "\n",
    "resid_te = hgb_final.predict(X_te_skl)\n",
    "if CALIB_MODE == \"ISO\":\n",
    "    test_pred = base_te.values + CALIB_PARAMS[\"iso\"].transform(resid_te)\n",
    "elif CALIB_MODE == \"AB\":\n",
    "    test_pred = base_te.values + CALIB_PARAMS[\"a\"] * resid_te + CALIB_PARAMS[\"b\"]\n",
    "else:\n",
    "    test_pred = base_te.values + resid_te\n",
    "\n",
    "test_pred = np.clip(test_pred, 0, None)\n",
    "\n",
    "# ---- 10) 저장\n",
    "sub = read_csv_smart(SAMPLE_SUB)\n",
    "sub['answer'] = pd.Series(test_pred, index=sub.index)\n",
    "sub.to_csv(OUT_SUB, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"[DONE] all_tr/all_te 재구성 → HGB 최종 예측 저장: {OUT_SUB}\")\n",
    "print(\"n_features:\", len(FEATS), \"  neg_rate:\", float((test_pred<0).mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fe2400a-ed8a-449e-9421-dc723365eca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 결측으로 무시되는 학습행: 0 / 204000\n",
      "[VAL] HGB RMSE=99.666\n",
      "[DONE] 정확도 회복 버전 저장 → C:\\Users\\user\\Downloads\\open (1)\\submission_rmse_residual.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# HGB 최종 재학습 (정확도 회복 버전)\n",
    "# - 백오프 베이스라인(g1→g2→g3→전체평균)\n",
    "# - 결측 y/베이스라인은 y_resid=0 + sample_weight=0 (드롭 없음)\n",
    "# - 기존 FEATS / X_full_skl / X_te_skl 없으면 즉석 생성\n",
    "# =========================================\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def RMSE(y, yhat): return float(np.sqrt(mean_squared_error(y, yhat)))\n",
    "\n",
    "# ---- 안전: 스키런 입력행렬이 없으면 즉석 생성\n",
    "if 'to_sklearn_matrix' not in globals():\n",
    "    def to_sklearn_matrix(X_tr: pd.DataFrame, X_te: pd.DataFrame, cat_cols):\n",
    "        Xtr = X_tr.copy(); Xte = X_te.copy()\n",
    "        use_cats = [c for c in (cat_cols or []) if c in Xtr.columns]\n",
    "        for c in use_cats:\n",
    "            Xtr[c] = Xtr[c].astype('category')\n",
    "            Xte[c] = Xte[c].astype('category').cat.set_categories(Xtr[c].cat.categories)\n",
    "            Xtr[c] = Xtr[c].cat.codes.replace(-1, np.nan)\n",
    "            Xte[c] = Xte[c].cat.codes.replace(-1, np.nan)\n",
    "        Xtr = Xtr.replace([np.inf,-np.inf], np.nan).astype(float).fillna(0.0)\n",
    "        Xte = Xte.replace([np.inf,-np.inf], np.nan).astype(float).fillna(0.0)\n",
    "        return Xtr, Xte\n",
    "\n",
    "EXCLUDE = {'전력소비량(kWh)','dt','날짜','시간','일시','num_date_time'}\n",
    "if 'FEATS' not in globals():\n",
    "    from pandas.api.types import is_numeric_dtype\n",
    "    FEATS = [c for c in all_tr.columns if c not in EXCLUDE and (is_numeric_dtype(all_tr[c]) or str(all_tr[c].dtype)=='category')]\n",
    "if 'cat_cols_use' not in globals():\n",
    "    cat_cols_use = [c for c in ['건물번호','건물유형'] if c in FEATS]\n",
    "if 'X_full_skl' not in globals() or 'X_te_skl' not in globals():\n",
    "    X_full_skl, X_te_skl = to_sklearn_matrix(all_tr[FEATS], all_te[FEATS], cat_cols_use)\n",
    "\n",
    "# ---- 1) 백오프 베이스라인\n",
    "g1 = all_tr.groupby(['건물번호','hour','weekday'])['전력소비량(kWh)'].mean()\n",
    "g2 = all_tr.groupby(['건물번호','hour'])['전력소비량(kWh)'].mean()\n",
    "g3 = all_tr.groupby(['건물번호'])['전력소비량(kWh)'].mean()\n",
    "g4 = float(all_tr['전력소비량(kWh)'].mean())\n",
    "\n",
    "def map_baseline(df: pd.DataFrame):\n",
    "    idx1 = df.set_index(['건물번호','hour','weekday']).index\n",
    "    s = pd.Series(g1.reindex(idx1).to_numpy(), index=df.index)\n",
    "    s = s.fillna(pd.Series(df.set_index(['건물번호','hour']).index.map(g2), index=df.index))\n",
    "    s = s.fillna(df['건물번호'].map(g3))\n",
    "    s = s.fillna(g4)\n",
    "    return s.astype(float)\n",
    "\n",
    "base_full = map_baseline(all_tr)\n",
    "base_te   = map_baseline(all_te)\n",
    "\n",
    "# ---- 2) 잔차 타깃 + 샘플가중치(결측은 가중치 0으로 무시)\n",
    "y_true = all_tr['전력소비량(kWh)']\n",
    "mask_ok = y_true.notna() & base_full.notna()\n",
    "y_resid = (y_true - base_full).where(mask_ok, 0.0).astype(float)  # 결측이면 0으로 채우되,\n",
    "w_full  = np.where(mask_ok, 1.0, 0.0)                             # 학습 영향은 0(weight)\n",
    "\n",
    "print(f\"[INFO] 결측으로 무시되는 학습행: {(w_full==0).sum()} / {len(w_full)}\")\n",
    "\n",
    "# ---- 3) HGB 하이퍼파라미터 (랜덤서치 베스트 있으면 반영)\n",
    "best_hp = {'learning_rate':0.06,'max_leaf_nodes':31,'l2_regularization':0.0,'max_iter':3500}\n",
    "if 'hgb_best' in globals() and isinstance(hgb_best.get('params', {}), dict):\n",
    "    best_hp.update(hgb_best['params'])\n",
    "\n",
    "hgb_final = HistGradientBoostingRegressor(\n",
    "    early_stopping=False, random_state=42, **best_hp\n",
    ")\n",
    "hgb_final.fit(X_full_skl, y_resid, sample_weight=w_full)\n",
    "\n",
    "# ---- 4) (옵션) VAL 체크\n",
    "if 'is_val' in globals():\n",
    "    va_idx = np.where(is_val.values)[0]\n",
    "    y_va_chk    = y_true.iloc[va_idx].fillna(base_full.iloc[va_idx])  # 결측이면 베이스라인으로 대체\n",
    "    base_va_chk = base_full.iloc[va_idx]\n",
    "    pred_resid  = hgb_final.predict(X_full_skl.iloc[va_idx])\n",
    "    yhat_va     = np.clip(base_va_chk.values + pred_resid, 0, None)\n",
    "    print(f\"[VAL] HGB RMSE={RMSE(y_va_chk, yhat_va):.3f}\")\n",
    "\n",
    "# ---- 5) 보정 적용 (있으면)\n",
    "CALIB_MODE   = globals().get('CALIB_MODE', 'NONE')\n",
    "CALIB_PARAMS = globals().get('CALIB_PARAMS', {})\n",
    "\n",
    "resid_te = hgb_final.predict(X_te_skl)\n",
    "if CALIB_MODE == \"ISO\":\n",
    "    test_pred = base_te.values + CALIB_PARAMS[\"iso\"].transform(resid_te)\n",
    "elif CALIB_MODE == \"AB\":\n",
    "    test_pred = base_te.values + CALIB_PARAMS[\"a\"] * resid_te + CALIB_PARAMS[\"b\"]\n",
    "else:\n",
    "    test_pred = base_te.values + resid_te\n",
    "\n",
    "test_pred = np.clip(test_pred, 0, None)\n",
    "\n",
    "# ---- 6) 저장\n",
    "sub = read_csv_smart(SAMPLE_SUB)\n",
    "sub['answer'] = pd.Series(test_pred, index=sub.index)\n",
    "sub.to_csv(OUT_SUB, index=False, encoding='utf-8-sig')\n",
    "print(f\"[DONE] 정확도 회복 버전 저장 → {OUT_SUB}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad62a6-480a-4838-b91d-3c67a8dde3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a770b-2832-4669-bd34-cfc8f920ab93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82306d-9a7e-4229-b801-19aa6fecc4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b464afa-f0a2-4ebf-9165-97e099888936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb3a62-347b-453d-8824-c046ee4f9621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7b2ba-6046-438a-a9fa-20cd470629c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87829ab-dbaf-4aae-8463-23ed91e22429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f761f0-ee20-443d-8243-f968ff5f9e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64922f9b-8d5f-4713-810f-a505bf962ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8240249-1012-4cee-aeec-bd57192b46c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081db1a6-b0b1-4d7b-8b1d-90cae08e3ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756e9ce-34cb-4b6c-9c61-3c125e23bc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df99ea0-29b6-46dc-a379-83b6bdecffc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05bf58-eda0-4b1a-8e29-d40e2cb99067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee882432-ae3a-49f1-95b7-0c5754955f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f5b37-9124-4ff2-a207-045a16a46be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7645f8d-d081-429f-96f4-d842d3026fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665d986-d667-4306-9b61-c4b8c5170d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169bad3-f477-4ac3-8470-4bfc1c6c92d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20aa403-144c-4ffc-a126-006cf39402e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a37bda-cbb4-4cef-96cc-d60ac7690ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e86397b-0abc-41e8-aadb-214364d9acca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f58370-1107-4ebf-8a08-7eb65e34e430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336cd93-91ee-4c78-919b-a2fddd6060e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7666f-9cb4-4913-8b44-f4ca38367375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95083ac0-0380-4132-a0ea-7e0d0087dca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4756793-a2ba-4d16-8bc3-91f89239e8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff8fd2-4a58-447a-b620-2d6aa0c09d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b96340-e246-44f8-a3b4-86717f2fcd37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cfe8e4-b323-483a-9ab0-20c594d01411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd119b4-8bc7-4606-95bf-18b99447e980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab054c-c929-4c1e-87d7-abcb876f9139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90f11b-d460-4bc3-917a-a4808865b17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a894c-b589-4323-8c9a-dfbb0624e09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b891cc7-b846-45bb-87b2-460d8726f7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694ae0b-3a01-45b7-9a7e-7096eef2ca61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f96069-a4b9-4cfe-ab82-e783d72e4f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec376e55-5750-46c8-9512-b16e9aa92279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ad073-deda-4d21-b869-3da771a8d535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc21fa7d-b790-4777-b8a2-3c06df00bbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89696f-e7e0-46bc-9fdb-748d86c7ac94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27901a1-9b7d-460d-bc67-5c339d6bf830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c2c43-48ad-4b33-93ff-087563111779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb8d4e-8b90-49b4-aa52-9efb9bf42e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66430db1-371f-4f97-8bb0-c971b12316c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a33289-90b4-46f6-8c3c-61be23eb892d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee9d839-2b44-493f-8ad6-c923b748a7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8041f2-3e2a-41f1-989c-095646de9a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa605a-4336-4d8f-b11b-b78405048012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea9fce5-0e59-4bd3-b9ab-81c035168b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae8f70-25ae-4ace-8daf-196a8ddfb73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37776b-bbda-474c-ace4-884c47606b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84920142-ca42-4150-b426-13f7e72fd7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639fb314-3101-4bd0-96f6-4ad4bfd210cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7aac67-e287-4ec7-af2e-b8a1cad7ac07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08361b5e-6a9c-4c53-ac09-41bcc28dbe69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12846b-5afe-4b09-a0eb-8d209ff21b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae95fd0-7a25-4f32-8bfa-0141214e3eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1848e-85ef-475d-b423-359f5e3706db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96b4a0-9547-4926-ae36-e5d4133da7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92177b-49f6-4d6d-9ac2-dcb0c7010742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b307be3-fdaf-4753-98dd-033f1979130d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "784b5a5e-2d98-4946-bb7e-71f8dfd960e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "df.to_csv(r\"C:\\Users\\user\\Downloads\\open (1)\\merged_train.csv\", index=False, encoding = 'cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0dcff9-f863-479e-bd76-10ef2b46fa3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b04e89-5b47-4b60-afc8-ba3c93d26af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
